{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from utils import eval, count_parameters\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# data\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# training\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the fields\n",
    "# source language is German and target language is \n",
    "SRC = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True,\n",
    "    include_lengths=True\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en\",\n",
    "    batch_first=True,\n",
    "#     include_lengths=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "train, val, test = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "SRC.build_vocab(train)\n",
    "TRG.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_loader, val_loader, test_loader = BucketIterator.splits(\n",
    "    datasets=(train, val, test),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x : len(x.src),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, src_len = batch.src\n",
    "trg =  batch.trg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Hidden Size of Encoder and Decoder recurrent net will be same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # reccurent layer\n",
    "        self.gru  = nn.GRU(\n",
    "            input_size = embedding_dim, \n",
    "            hidden_size=hidden_size,\n",
    "            num_layers = 1,\n",
    "            batch_first = True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # forward and backward output will be stacked\n",
    "        self.fc = nn.Linear(in_features=2*hidden_size, out_features=hidden_size)\n",
    "    \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        embedded =  self.embedding(src)\n",
    "        \n",
    "        rnn_input = nn.utils.rnn.pack_padded_sequence(input=embedded, lengths=src_len, batch_first=True)\n",
    "        packed_outputs, hidden = self.gru(rnn_input)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        \n",
    "        # hidden contains both forward pass hidden state as well backward pass hidden state concat the both\n",
    "        concated = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        \n",
    "        # new hidden state \n",
    "        hidden = torch.tanh(self.fc(concated))\n",
    "        \n",
    "        return outputs, hidden\n",
    "        \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # attention layers\n",
    "        self.attention = nn.Linear(in_features=3*hidden_size, out_features=hidden_size)\n",
    "        self.v = nn.Linear(in_features=hidden_size, out_features=1, bias=False)\n",
    "        \n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        \n",
    "        \"\"\"\n",
    "            mask is basically pad token\n",
    "            hidden.shape -> [batch, hidden_size]\n",
    "            encoder_outputs.shape -> [batch, seq_len, 2*hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # repeat the hidden state upto T(src_len) times but first add the additional axis\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        # concate the hidden state and encoder_outputs and pass to the attention module to calculate the energy\n",
    "        energy = torch.tanh(self.attention(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy.shape -> [batch, seq_len, hidden_size]\n",
    "        \n",
    "        attention = self.v(energy).squeeze()\n",
    "        # attention.shape -> [batch, seq_len]\n",
    "        \n",
    "#         attention = attention.masked_fill(mask==0, -1e10)\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, attention, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = attention\n",
    "        \n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        # reccurent net\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=(2*hidden_size)+embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(in_features=3*hidden_size+embedding_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        \"\"\"\n",
    "            input.size() -> [batch, 1] - At time t only one token of each sample will be decoded\n",
    "            hidden.size() ->[batch, hidden_size]\n",
    "            encoder_outputs -> [batch, seq_len, 2*hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.embedding(input.to(device))\n",
    "        # embedded[batch, seq_len, embedding_dim] embedded will have only two dim if seq_len is 1 (ie: at time t)\n",
    "             \n",
    "        attn = self.attention(hidden, encoder_outputs)\n",
    "        attn = attn.unsqueeze(1)\n",
    "    \n",
    "        # calculate the weighted sum\n",
    "        weighted = torch.bmm(attn, encoder_outputs)\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "#         print(f'weighted shape {weighted.shape} embedded shape {embedded.shape}')\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        # prepare the input for fully connected layer and make predicitons\n",
    "        fc_input = torch.cat((weighted, output, embedded), dim=2)\n",
    "        prediction = self.fc(fc_input)\n",
    "        \n",
    "        return prediction, hidden.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio=0.25):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \n",
    "        src, src_len = batch.src\n",
    "        trg = batch.trg\n",
    "        \n",
    "        \"\"\"\n",
    "            trg.size() -> [batch, seq_len]\n",
    "            src.size() -> [batch, seq_len]\n",
    "        \"\"\"\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        \n",
    "        batch, seq_len, vocab_size =  trg.shape[0], trg.shape[1], self.decoder.vocab_size\n",
    "        \n",
    "        outputs = torch.zeros((batch, seq_len, vocab_size), device=device)\n",
    "        \n",
    "        # take the first token of each samples in the batch and calculate the attention for the same\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            output, hidden = decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output.squeeze()\n",
    "\n",
    "            # is teacher force\n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.squeeze().argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data, criterion):\n",
    "    loss, ppl = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data:\n",
    "            outputs = model(batch)\n",
    "            batch_size, seq_len = outputs.size(0), outputs.size(1)\n",
    "            l = criterion(outputs.view(batch_size*seq_len, -1).contiguous().to(device), batch.trg.view(-1))\n",
    "            p = torch.exp(l)\n",
    "            loss.append(l.item())\n",
    "            ppl.append(p.item())\n",
    "    return sum(loss)/len(loss), sum(ppl)/len(ppl)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = len(SRC.vocab)\n",
    "trg_vocab = len(TRG.vocab)\n",
    "hidden_size = 512 # same for encoder and decoder\n",
    "embedding_dim =  256 # same for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder =  Encoder(vocab_size=src_vocab, embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)\n",
    "attention = Attention(hidden_size=hidden_size).to(device)\n",
    "decoder = Decoder(attention, vocab_size=trg_vocab, embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)\n",
    "model = Seq2Seq(encoder=encoder, decoder=decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(18660, 256)\n",
       "    (gru): GRU(256, 512, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attention): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(9799, 256)\n",
       "    (gru): GRU(1280, 512, batch_first=True)\n",
       "    (fc): Linear(in_features=1792, out_features=9799, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------+\n",
      "|              Modules               | Parameters |\n",
      "+------------------------------------+------------+\n",
      "|      encoder.embedding.weight      |  4776960   |\n",
      "|      encoder.gru.weight_ih_l0      |   393216   |\n",
      "|      encoder.gru.weight_hh_l0      |   786432   |\n",
      "|       encoder.gru.bias_ih_l0       |    1536    |\n",
      "|       encoder.gru.bias_hh_l0       |    1536    |\n",
      "|  encoder.gru.weight_ih_l0_reverse  |   393216   |\n",
      "|  encoder.gru.weight_hh_l0_reverse  |   786432   |\n",
      "|   encoder.gru.bias_ih_l0_reverse   |    1536    |\n",
      "|   encoder.gru.bias_hh_l0_reverse   |    1536    |\n",
      "|         encoder.fc.weight          |   524288   |\n",
      "|          encoder.fc.bias           |    512     |\n",
      "| decoder.attention.attention.weight |   786432   |\n",
      "|  decoder.attention.attention.bias  |    512     |\n",
      "|     decoder.attention.v.weight     |    512     |\n",
      "|      decoder.embedding.weight      |  2508544   |\n",
      "|      decoder.gru.weight_ih_l0      |  1966080   |\n",
      "|      decoder.gru.weight_hh_l0      |   786432   |\n",
      "|       decoder.gru.bias_ih_l0       |    1536    |\n",
      "|       decoder.gru.bias_hh_l0       |    1536    |\n",
      "|         decoder.fc.weight          |  17559808  |\n",
      "|          decoder.fc.bias           |    9799    |\n",
      "+------------------------------------+------------+\n",
      "Total Trainable Params: 31288391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31288391"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 1e-3\n",
    "PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "total_steps = len(train_loader)*epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/2270 [00:00<04:55,  7.68it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/5 | Steps 454/2270 | Train_loss 5.3352 | Train_ppl 332.4269 | Val_loss 4.7795 | Val_ppl 122.9651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/2270 [00:00<06:16,  6.02it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Steps 908/2270 | Train_loss 4.2993 | Train_ppl 79.2242 | Val_loss 3.9038 | Val_ppl 52.2731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/2270 [00:00<05:18,  7.12it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 | Steps 1362/2270 | Train_loss 3.5430 | Train_ppl 36.4696 | Val_loss 3.5598 | Val_ppl 36.9900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/2270 [00:00<05:26,  6.95it/s]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 | Steps 1816/2270 | Train_loss 3.0317 | Train_ppl 21.8867 | Val_loss 3.4040 | Val_ppl 31.6249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 5/5 [05:40<00:00, 68.50s/it]it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 | Steps 2270/2270 | Train_loss 2.6168 | Train_ppl 14.4381 | Val_loss 3.3914 | Val_ppl 31.1631\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "\n",
    "# epoch progress bar\n",
    "epoch_progress = tqdm.tqdm(total=epochs, desc=\"Epoch\", position=0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # step progress bar\n",
    "    step_progress = tqdm.tqdm(total=total_steps, desc=\"Steps\", position=0)\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_ppl = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        \n",
    "        outputs = model(batch)\n",
    "        \n",
    "        batch_size, seq_len = outputs.shape[0], outputs.shape[1]\n",
    "        \n",
    "        outputs = outputs.view((batch_size*seq_len, -1))\n",
    "        labels = batch.trg.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        ppl = torch.exp(loss)\n",
    "        \n",
    "        # backprograpage the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_ppl.append(ppl.item())\n",
    "        \n",
    "#         if steps % 200 == 0:\n",
    "#             print(f'Steps {steps}/{total_steps} | Train_loss {loss.item():.4f} | Train_ppl {ppl.item():.4f}')\n",
    "        steps += 1\n",
    "        step_progress.update(1)\n",
    "    \n",
    "    avg_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    avg_ppl = sum(epoch_ppl)/len(epoch_ppl)\n",
    "    \n",
    "    val_loss, val_ppl = eval(model, val_loader, criterion)\n",
    "    print(f'Epoch {epoch}/{epochs} | Steps {steps}/{total_steps} | Train_loss {avg_loss:.4f} | Train_ppl {avg_ppl:.4f} | Val_loss {val_loss:.4f} | Val_ppl {val_ppl:.4f}')\n",
    "    epoch_progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss 3.3686 | Test_PPL 30.097521543502808\n"
     ]
    }
   ],
   "source": [
    "loss, ppl = eval(model, test_loader, criterion)\n",
    "print(f'Test_loss {loss:.4f} | Test_PPL {ppl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
