{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import string\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import unicodedata\n",
    "\n",
    "# data and model\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# training\n",
    "import tqdm\n",
    "import torch.utils.tensorboard as tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_letters  = string.ascii_letters + \".I-';,`!@#$%^&*\"\n",
    "n_letters = len(all_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    return glob.glob(path)\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names in different languges\n",
    "files = get_files('../data/names/*.txt')\n",
    "\n",
    "# read the names\n",
    "data = {}\n",
    "for file in files:\n",
    "    data[file.split(\"/\")[-1].split(\".\")[0]] = list(map(unicodeToAscii, open(file).read().splitlines()))\n",
    "\n",
    "# language\n",
    "classes = list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cls2idx and idx2cls dictionary \n",
    "cls2idx = dict(zip(classes, np.linspace(start=0, stop=len(classes)-1, num=len(classes), dtype=int)))\n",
    "idx2cls = dict(zip(cls2idx.values(), cls2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char2Idx(char):\n",
    "    return all_letters.find(char)\n",
    "\n",
    "def char2Tensor(char):\n",
    "    tensor = torch.zeros((1, n_letters))\n",
    "    tensor[0][char2Idx(char)] = 1\n",
    "    return tensor\n",
    "\n",
    "def name2Tensor(name):\n",
    "    tensor = torch.zeros((len(name), 1, n_letters))\n",
    "    for cidx, char in enumerate(name):\n",
    "        tensor[cidx][0][char2Idx(char)] = 1\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "labels = []\n",
    "for cls in classes:\n",
    "    i = data[cls]\n",
    "    l = [cls2idx[cls]]*len(i)\n",
    "    inputs = inputs + i\n",
    "    labels = labels + l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, inputs, labels):\n",
    "        self.x = inputs\n",
    "        self.y = labels\n",
    "        self.len = len(self.x)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_letters, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.i2h = nn.Linear(in_features=n_letters+hidden_size, out_features=hidden_size)\n",
    "        self.i2o = nn.Linear(in_features=n_letters+hidden_size, out_features=output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input.size() -> len(name), 1, n_letters\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "       \n",
    "        hidden = self.activation(self.i2h(combined))\n",
    "        outputs = self.activation(self.i2o(combined))\n",
    "        return hidden, outputs\n",
    "    \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros((1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(\n",
    "    n_letters=n_letters,\n",
    "    hidden_size=64,\n",
    "    output_size=len(classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, input):\n",
    "    hidden = model.init_hidden()\n",
    "    for i in range(input.size(0)):\n",
    "        hidden, outputs = model(input[i], hidden)\n",
    "    return nn.Softmax()(outputs)  \n",
    "\n",
    "def accuracy(y, y_):\n",
    "    correct = 0\n",
    "    for i in range(y.size(0)):\n",
    "        if y[i]==y_[i]:\n",
    "            correct += 1\n",
    "    return correct/y.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]/home/macab/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Steps 0 | Loss 2.7250\n",
      "Epoch 0 | Steps 100 | Loss 2.4845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 1/20 [00:27<08:33, 27.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Steps 200 | Loss 2.5588\n",
      "Epoch 1 | Steps 300 | Loss 2.4857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 2/20 [00:54<08:09, 27.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Steps 400 | Loss 2.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 3/20 [01:24<07:53, 27.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Steps 500 | Loss 2.7297\n",
      "Epoch 3 | Steps 600 | Loss 2.4694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 4/20 [01:53<07:34, 28.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Steps 700 | Loss 2.3178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 5/20 [02:20<07:00, 28.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Steps 800 | Loss 2.1893\n",
      "Epoch 5 | Steps 900 | Loss 2.2865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 6/20 [02:45<06:19, 27.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Steps 1000 | Loss 2.2263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  35%|███▌      | 7/20 [03:09<05:40, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Steps 1100 | Loss 2.2564\n",
      "Epoch 7 | Steps 1200 | Loss 2.3309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 8/20 [03:33<05:04, 25.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Steps 1300 | Loss 2.2182\n",
      "Epoch 8 | Steps 1400 | Loss 2.2685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|████▌     | 9/20 [03:57<04:35, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Steps 1500 | Loss 2.2113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 10/20 [04:23<04:13, 25.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Steps 1600 | Loss 2.3060\n",
      "Epoch 10 | Steps 1700 | Loss 2.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▌    | 11/20 [04:49<03:50, 25.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Steps 1800 | Loss 2.2691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 12/20 [05:16<03:27, 25.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Steps 1900 | Loss 2.2412\n",
      "Epoch 12 | Steps 2000 | Loss 2.2954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 13/20 [05:40<02:57, 25.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Steps 2100 | Loss 2.2227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 14/20 [06:06<02:33, 25.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Steps 2200 | Loss 2.2734\n",
      "Epoch 14 | Steps 2300 | Loss 2.2825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 15/20 [06:32<02:08, 25.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Steps 2400 | Loss 2.2720\n",
      "Epoch 15 | Steps 2500 | Loss 2.2853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 16/20 [06:57<01:41, 25.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 | Steps 2600 | Loss 2.2383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  85%|████████▌ | 17/20 [07:21<01:15, 25.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 | Steps 2700 | Loss 2.2483\n",
      "Epoch 17 | Steps 2800 | Loss 2.2147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 18/20 [07:45<00:49, 24.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 | Steps 2900 | Loss 2.1883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  95%|█████████▌| 19/20 [08:08<00:24, 24.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 | Steps 3000 | Loss 2.2287\n",
      "Epoch 19 | Steps 3100 | Loss 2.2353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [08:32<00:00, 24.11s/it]"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "epoch_progress = tqdm.tqdm(total=epochs, desc=\"Epoch\", position=0)\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    for x, y in data_loader:\n",
    "        outputs = torch.empty((len(x), len(classes)))\n",
    "        for i, name in enumerate(x):\n",
    "            outputs[i] = fit(model, name2Tensor(name))\n",
    "        \n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % 100==0:\n",
    "            print(f'Epoch {epoch} | Steps {steps} | Loss {loss.item():.4f}')\n",
    "        steps += 1\n",
    "    \n",
    "    epoch_progress.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2cls[y[i].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Balarev'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "out = fit(model, name2Tensor(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
