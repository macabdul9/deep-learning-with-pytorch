{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from utils import count_parameters\n",
    "import torch\n",
    "\n",
    "# data\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# training\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data fields for source and target\n",
    "source = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True\n",
    ")\n",
    "target = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the parallel corpus\n",
    "train, val, test = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(source, target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "source.build_vocab(train)\n",
    "target.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader, val_loader, test_loader = BucketIterator.splits(\n",
    "    datasets=(train, val, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 28]) torch.Size([128, 32])\n"
     ]
    }
   ],
   "source": [
    "batch =  next(iter(train_loader))\n",
    "print(batch.src.shape, batch.trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        transformer encoder module returns a [batch_size, seq_len, out_dim] tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, n_heads, pf_dim, dropout=0.15, max_len=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # tok and pos embedding dim is same because we have to add them\n",
    "        self.tok_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=embedding_dim)\n",
    "        \n",
    "        \n",
    "        # encoder layers of transformer encoder module\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(embedding_dim, n_heads, pf_dim, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # scaling\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
    "        \n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "            src.shape -> [batch, src_len]\n",
    "            src_mask -> [batch, src_len]\n",
    "        \"\"\"\n",
    "        batch, src_len = src.shape[0], src.shape[1]\n",
    "        \n",
    "        # create position tensor, shape will be [batch, src_len] by dooing so batch_first will be True\n",
    "        position  = torch.arange(start=0, end=src_len, device=device).unsqueeze(0).repeat(batch, 1)\n",
    "        \n",
    "        # embeddings\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(position)\n",
    "        \n",
    "        # scale the token embeddings by multiplyig it with srqt(d_model) where d_model is embedding_dim\n",
    "        tok_scaled = tok_embedded * self.scale.to(device)\n",
    "        \n",
    "        # add the scaled_tok and position embedding and then apply dropout, that will be input to the encoder\n",
    "        encoder_input = self.dropout(tok_scaled + pos_embedded)\n",
    "        \n",
    "        \n",
    "        #feed the input to the encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(encoder_input, None)\n",
    "        \n",
    "        return src\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, n_heads, pf_dim, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # layer normalization\n",
    "        self.layer_norm =  nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # multi-head attention (I love this layer)\n",
    "        self.multihead_attention = MultiheadAttention(embedding_dim, n_heads, dropout)\n",
    "                \n",
    "        # feedforward layer\n",
    "        self.positionwise_ff = PositionwiseFeedForwardLayer(embedding_dim, pf_dim, dropout)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        \n",
    "        # compute the attention values (query, key, value) -> (src, src, src)\n",
    "        attn_out, _  = self.multihead_attention(src, src, src, src_mask)\n",
    "        \n",
    "        \n",
    "        # Normalize the attention and build residual connection and then pass it to positionwise ff layer followed by LN\n",
    "        attn_norm_out = self.layer_norm(src + self.dropout(attn_out))\n",
    "        ff_out = self.positionwise_ff(attn_norm_out)\n",
    "        ff_norm_out = self.layer_norm(attn_norm_out + self.dropout(ff_out)) \n",
    "        # this will be output of the Transformer's Encoder layer\n",
    "        # ff_norm_out.shape [batch, seq_len, embedding_dim]\n",
    "        \n",
    "        return ff_norm_out     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Scaled dot product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, n_heads, dropout):\n",
    "        \"\"\"\n",
    "            n_heads > 0\n",
    "        \"\"\"\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embedding_dim // n_heads\n",
    "        \n",
    "        # fc for key, query, values\n",
    "        self.fc_k  = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.fc_q = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.fc_v = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.fc_o = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "            query.shape -> [batch, src_len, embedding_dim]\n",
    "            key.shape -> [batch, src_len, embedding_dim]\n",
    "            value.shape -> [batch, src_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        src_len = query.shape[1]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        #Q = [batch size, query len, hid dim] K&V would have same dim\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch size, n heads, query len, head dim] K&V have to have same dim\n",
    "        \n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1) \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, embedding_dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.embedding_dim)\n",
    "        #x = [batch size, query len, embedding_dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, embedding_dim]\n",
    "        \n",
    "        return x, attention    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positionwise Feedforad Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, pf_dim, dropout):\n",
    "        super(PositionwiseFeedForwardLayer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=embedding_dim, out_features=pf_dim)\n",
    "        self.fc2 = nn.Linear(in_features=pf_dim, out_features=embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            x.shape -> [batch, src_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        out = self.fc2(self.dropout(F.relu((self.fc1(x)))))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer's Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        transformer encoder module returns a [batch_size, seq_len, out_dim] tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, n_heads, pf_dim, dropout=0.15, max_len=100):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # tok and pos embedding dim is same because we have to add them\n",
    "        self.tok_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=embedding_dim)\n",
    "        \n",
    "        \n",
    "        # encoder layers of transformer encoder module\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(embedding_dim, n_heads, pf_dim, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(embedding_dim, out_features=vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # scaling\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
    "        \n",
    "    \n",
    "    def forward(self, trg, enc_src, trg_mask=None, src_mask=None):\n",
    "\n",
    "        \"\"\"\n",
    "            trg is target tokens and enc_src is encoder\n",
    "            \n",
    "            trg.shape -> [batch, trg_len]\n",
    "            enc_src.shape -> [batch, src_len, embedding_dim] # This is why it is recommended to use same embedding dim\n",
    "            \n",
    "        \"\"\"\n",
    "        batch, trg_len = trg.shape[0], trg.shape[1]\n",
    "        \n",
    "        \n",
    "        # create position tensor, shape will be [batch, src_len] by dooing so batch_first will be True\n",
    "        position  = torch.arange(start=0, end=trg_len, device=device).unsqueeze(0).repeat(batch, 1)\n",
    "        \n",
    "        # embeddings\n",
    "        tok_embedded = self.tok_embedding(trg)\n",
    "        pos_embedded = self.pos_embedding(position)\n",
    "        \n",
    "        \n",
    "        # scale the token embeddings by multiplyig it with srqt(d_model) where d_model is embedding_dim\n",
    "        tok_scaled = tok_embedded * self.scale.to(device)\n",
    "        \n",
    "        # add the scaled_tok and position embedding and then apply dropout, that will be input to the encoder\n",
    "        decoder_input = self.dropout(tok_scaled + pos_embedded)\n",
    "        \n",
    "        \n",
    "        #feed the input to the encoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            trg, attention = layer(decoder_input, enc_src, None, None)\n",
    "        \n",
    "        \n",
    "        outputs = self.fc_out(trg)\n",
    "        \n",
    "        return outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding_dim, \n",
    "                 n_heads, \n",
    "                 pf_dim, \n",
    "                 dropout, \n",
    "                 ):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        self.self_attention = MultiheadAttention(embedding_dim, n_heads, dropout)\n",
    "        self.encoder_attention = MultiheadAttention(embedding_dim, n_heads, dropout)\n",
    "        \n",
    "        self.positionwise_feedforward = PositionwiseFeedForwardLayer(embedding_dim,  pf_dim,  dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, trg len]\n",
    "        #src_mask = [batch size, src len]\n",
    "        \n",
    "        #self attention\n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "            \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "            \n",
    "        #encoder attention\n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        \n",
    "        #dropout, residual connection and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "                    \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        \n",
    "        #positionwise feedforward\n",
    "        _trg = self.positionwise_feedforward(trg)\n",
    "        \n",
    "        #dropout, residual and layer norm\n",
    "        trg = self.layer_norm(trg + self.dropout(_trg))\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = target.vocab.stoi[target.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining all modules into a PyTorch-Lightining Tranformer Module\n",
    "\n",
    "class Transformer(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        src_encoded = self.encoder(src, None)\n",
    "        outputs = self.decoder(trg, src_encoded, None, None)\n",
    "        return outputs\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(params=self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return train_loader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        src, trg = batch.src, batch.trg\n",
    "        batch_size, trg_len = trg.shape[0], trg.shape[1]\n",
    "        outputs  = self(src, trg)\n",
    "        outputs = outputs.view(batch_size*trg_len, -1)\n",
    "        loss =  F.cross_entropy(outputs, trg.view(-1), ignore_index=PAD_IDX)\n",
    "        ppl = torch.exp(loss)\n",
    "        tensorboard_logs = {\"loss\":loss, \"ppl\":ppl}\n",
    "        return {\"loss\":loss, \"ppl\":ppl, \"log\":tensorboard_logs}\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return val_loader\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        src, trg = batch.src, batch.trg\n",
    "        batch_size, trg_len = trg.shape[0], trg.shape[1]\n",
    "        outputs = self(src, trg)\n",
    "        outputs = outputs.view(batch_size*trg_len, -1)\n",
    "        loss =  F.cross_entropy(outputs, trg.view(-1), ignore_index=PAD_IDX)\n",
    "        ppl = torch.exp(loss)\n",
    "#         tensorboard_logs = {\"val_loss\":loss, \"val_ppl\":ppl}\n",
    "        return {\"val_loss\":loss, \"val_ppl\":ppl}\n",
    "\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_ppl = torch.stack([x['val_ppl'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {'val_loss': avg_loss, 'val_ppl':avg_ppl}\n",
    "        return {'val_loss': avg_loss,'val_ppl':avg_ppl, 'log': tensorboard_logs}\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = len(source.vocab)\n",
    "trg_vocab = len(target.vocab)\n",
    "embedding_dim = 256\n",
    "n_heads = 8\n",
    "num_layers = 1\n",
    "pf_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size=src_vocab, embedding_dim=embedding_dim, num_layers=num_layers, n_heads=n_heads, pf_dim=pf_dim)\n",
    "decoder = Decoder(vocab_size=trg_vocab, embedding_dim=embedding_dim, num_layers=num_layers, n_heads=n_heads, pf_dim=pf_dim)\n",
    "transformer = Transformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "# see gpus/tpu params if you want to train on gpus/tpu \n",
    "trainer = pl.Trainer(max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | encoder | Encoder | 5 M   \n",
      "1 | decoder | Decoder | 5 M   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  97%|█████████▋| 227/235 [02:39<00:05,  1.43it/s, loss=0.341, v_num=24]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  97%|█████████▋| 228/235 [02:39<00:04,  1.43it/s, loss=0.341, v_num=24]\n",
      "Epoch 1:  98%|█████████▊| 230/235 [02:39<00:03,  1.44it/s, loss=0.341, v_num=24]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00,  9.42it/s]\u001b[A\n",
      "Epoch 1:  99%|█████████▊| 232/235 [02:39<00:02,  1.45it/s, loss=0.341, v_num=24]\n",
      "Validating:  75%|███████▌  | 6/8 [00:00<00:00,  8.58it/s]\u001b[A\n",
      "Epoch 1: 100%|█████████▉| 234/235 [02:39<00:00,  1.46it/s, loss=0.341, v_num=24]\n",
      "Epoch 1: 100%|██████████| 235/235 [02:40<00:00,  1.47it/s, loss=0.341, v_num=24]\n",
      "Epoch 2:  97%|█████████▋| 227/235 [03:04<00:06,  1.23it/s, loss=0.135, v_num=24]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2:  97%|█████████▋| 228/235 [03:04<00:05,  1.24it/s, loss=0.135, v_num=24]\n",
      "Validating:  25%|██▌       | 2/8 [00:00<00:00,  8.34it/s]\u001b[A\n",
      "Epoch 2:  98%|█████████▊| 230/235 [03:04<00:04,  1.25it/s, loss=0.135, v_num=24]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00,  7.35it/s]\u001b[A\n",
      "Epoch 2:  99%|█████████▊| 232/235 [03:04<00:02,  1.25it/s, loss=0.135, v_num=24]\n",
      "Validating:  75%|███████▌  | 6/8 [00:00<00:00,  6.48it/s]\u001b[A\n",
      "Epoch 2: 100%|█████████▉| 234/235 [03:05<00:00,  1.26it/s, loss=0.135, v_num=24]\n",
      "Epoch 2: 100%|██████████| 235/235 [03:05<00:00,  1.26it/s, loss=0.135, v_num=24]\n",
      "Epoch 3:  97%|█████████▋| 227/235 [02:58<00:06,  1.27it/s, loss=0.069, v_num=24]\n",
      "Epoch 3:  97%|█████████▋| 228/235 [02:58<00:05,  1.28it/s, loss=0.069, v_num=24]\n",
      "Validating:  25%|██▌       | 2/8 [00:00<00:00, 11.50it/s]\u001b[A\n",
      "Epoch 3:  98%|█████████▊| 230/235 [02:58<00:03,  1.29it/s, loss=0.069, v_num=24]\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00,  8.83it/s]\u001b[A\n",
      "Epoch 3:  99%|█████████▊| 232/235 [02:58<00:02,  1.30it/s, loss=0.069, v_num=24]\n",
      "Validating:  75%|███████▌  | 6/8 [00:00<00:00,  7.81it/s]\u001b[A\n",
      "Epoch 3: 100%|█████████▉| 234/235 [02:59<00:00,  1.30it/s, loss=0.069, v_num=24]\n",
      "Epoch 3: 100%|██████████| 235/235 [02:59<00:00,  1.31it/s, loss=0.069, v_num=24]\n",
      "Epoch 4:   4%|▍         | 9/235 [00:08<03:25,  1.10it/s, loss=0.057, v_num=24]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:25: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:  12%|█▎        | 1/8 [00:00<00:00,  9.70it/s]\u001b[A\n",
      "Validating:  25%|██▌       | 2/8 [00:00<00:00,  9.74it/s]\u001b[A\n",
      "Validating:  38%|███▊      | 3/8 [00:00<00:00,  9.72it/s]\u001b[A\n",
      "Validating:  50%|█████     | 4/8 [00:00<00:00,  9.39it/s]\u001b[A\n",
      "Validating:  62%|██████▎   | 5/8 [00:00<00:00,  9.34it/s]\u001b[A\n",
      "Validating:  75%|███████▌  | 6/8 [00:00<00:00,  8.79it/s]\u001b[A\n",
      "Validating:  88%|████████▊ | 7/8 [00:00<00:00,  8.18it/s]\u001b[A\n",
      "Validating: 100%|██████████| 8/8 [00:00<00:00,  7.20it/s]\u001b[A\n",
      "                                                         \u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_loss': tensor(0.1275), 'val_ppl': tensor(1.1361)}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Some codes are directly taken from here: \n",
    "- https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
