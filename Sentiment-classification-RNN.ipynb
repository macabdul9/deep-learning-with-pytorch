{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/tqdm/std.py:668: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "# data preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "tqdm.pandas()\n",
    "\n",
    "# model \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source: \n",
    "- https://colab.research.google.com/github/agungsantoso/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Exercise.ipynb#scrollTo=bpZdIItNpmwU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50000\n",
    "max_len = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some part of the cleaing code is taken from here:  https://www.kaggle.com/hengzheng/pytorch-starter\n",
    "#### Cleaning steps\n",
    "- Remove Punctuation\n",
    "- Remote numbers and special chars\n",
    "- Padding and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleaning import clean_numbers, clean_text, replace_typical_misspell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean():\n",
    "    train = pd.read_csv(\"data/quora/train.csv\", nrows=100000, usecols=[\"question_text\", \"target\"])\n",
    "    \n",
    "    # convert it into lower \n",
    "    train[\"text\"] = train[\"question_text\"].progress_apply(lambda x : x.lower())\n",
    "    \n",
    "    # clean the text by removing special chars, numbers etc\n",
    "    train[\"text\"] = train[\"text\"].progress_apply(lambda x : clean_text(x))\n",
    "    train[\"text\"] = train[\"text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "    \n",
    "    # fix the spelling\n",
    "    train[\"text\"] = train[\"text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:00<00:00, 1041724.06it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 38794.41it/s]\n",
      "100%|██████████| 100000/100000 [00:01<00:00, 99822.29it/s]\n",
      "100%|██████████| 100000/100000 [00:02<00:00, 45013.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train = load_and_clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question_text', 'target', 'text'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = train.text.values\n",
    "targets = train.target.values\n",
    "corpus = \" \".join(text)\n",
    "words = corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how',\n",
       " 'did',\n",
       " 'quebec',\n",
       " 'nationalists',\n",
       " 'see',\n",
       " 'their',\n",
       " 'province',\n",
       " 'as',\n",
       " 'a',\n",
       " 'nation']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "word2idx = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = []\n",
    "for ques in text:\n",
    "    encoded.append([word2idx[word] for word in ques.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words : 50475\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx)\n",
    "print(f\"Unique words : {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " how did quebec nationalists see their province as a nation in the ####s ?\n"
     ]
    }
   ],
   "source": [
    "ques = \"\"\n",
    "for token in encoded[0]:\n",
    "    ques = ques + \" \" + idx2word[token]\n",
    "print(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min len 1\n",
      "max len 143\n"
     ]
    }
   ],
   "source": [
    "#### Min and Max len of Sequence so that we can pad the encoded sequence\n",
    "seq_lens = [len(seq) for seq in encoded]\n",
    "print(f\"min len {min(seq_lens)}\\nmax len {max(seq_lens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(sequences, seq_length=100):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    ## getting the correct rows x cols shape\n",
    "    features = np.zeros((len(sequences), seq_length), dtype=int)\n",
    "    \n",
    "    ## for each review, I grab that review\n",
    "    for i, seq in enumerate(sequences):\n",
    "        features[i, -len(seq):] = np.array(seq)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "padded_sequence = pad_features(encoded, seq_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequence, targets, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95000, 100) (5000, 100) (95000,) (5000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "valid_data = torch.utils.data.TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "          hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                   weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(50476, 400)\n",
      "  (lstm): LSTM(400, 256, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(word2idx) + 1 # +1 for zero padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 400 \n",
    "hidden_dim = 256\n",
    "n_layers = 1\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 100... Loss: 0.123443... Val Loss: 0.118740\n",
      "Epoch: 1/4... Step: 200... Loss: 0.088895... Val Loss: 0.125022\n",
      "Epoch: 1/4... Step: 300... Loss: 0.041370... Val Loss: 0.120382\n",
      "Epoch: 1/4... Step: 400... Loss: 0.121991... Val Loss: 0.120787\n",
      "Epoch: 1/4... Step: 500... Loss: 0.038624... Val Loss: 0.126568\n",
      "Epoch: 1/4... Step: 600... Loss: 0.102510... Val Loss: 0.122921\n",
      "Epoch: 1/4... Step: 700... Loss: 0.088225... Val Loss: 0.122281\n",
      "Epoch: 1/4... Step: 800... Loss: 0.063609... Val Loss: 0.121687\n",
      "Epoch: 1/4... Step: 900... Loss: 0.068324... Val Loss: 0.121364\n",
      "Epoch: 1/4... Step: 1000... Loss: 0.032588... Val Loss: 0.122497\n",
      "Epoch: 1/4... Step: 1100... Loss: 0.040477... Val Loss: 0.120436\n",
      "Epoch: 1/4... Step: 1200... Loss: 0.197443... Val Loss: 0.118960\n",
      "Epoch: 1/4... Step: 1300... Loss: 0.224305... Val Loss: 0.122955\n",
      "Epoch: 1/4... Step: 1400... Loss: 0.064327... Val Loss: 0.124899\n",
      "Epoch: 1/4... Step: 1500... Loss: 0.091575... Val Loss: 0.121040\n",
      "Epoch: 1/4... Step: 1600... Loss: 0.063840... Val Loss: 0.120820\n",
      "Epoch: 1/4... Step: 1700... Loss: 0.095410... Val Loss: 0.121002\n",
      "Epoch: 1/4... Step: 1800... Loss: 0.101282... Val Loss: 0.120196\n",
      "Epoch: 1/4... Step: 1900... Loss: 0.050613... Val Loss: 0.120535\n",
      "Epoch: 1/4... Step: 2000... Loss: 0.139123... Val Loss: 0.117274\n",
      "Epoch: 1/4... Step: 2100... Loss: 0.064745... Val Loss: 0.117818\n",
      "Epoch: 1/4... Step: 2200... Loss: 0.074375... Val Loss: 0.117353\n",
      "Epoch: 1/4... Step: 2300... Loss: 0.093925... Val Loss: 0.121539\n",
      "Epoch: 1/4... Step: 2400... Loss: 0.208608... Val Loss: 0.122096\n",
      "Epoch: 1/4... Step: 2500... Loss: 0.098560... Val Loss: 0.122826\n",
      "Epoch: 1/4... Step: 2600... Loss: 0.134233... Val Loss: 0.119237\n",
      "Epoch: 1/4... Step: 2700... Loss: 0.075415... Val Loss: 0.123400\n",
      "Epoch: 1/4... Step: 2800... Loss: 0.192531... Val Loss: 0.120631\n",
      "Epoch: 1/4... Step: 2900... Loss: 0.070130... Val Loss: 0.119943\n",
      "Epoch: 2/4... Step: 3000... Loss: 0.060512... Val Loss: 0.120566\n",
      "Epoch: 2/4... Step: 3100... Loss: 0.056493... Val Loss: 0.129702\n",
      "Epoch: 2/4... Step: 3200... Loss: 0.003299... Val Loss: 0.132025\n",
      "Epoch: 2/4... Step: 3300... Loss: 0.011453... Val Loss: 0.132549\n",
      "Epoch: 2/4... Step: 3400... Loss: 0.173379... Val Loss: 0.130360\n",
      "Epoch: 2/4... Step: 3500... Loss: 0.032392... Val Loss: 0.133544\n",
      "Epoch: 2/4... Step: 3600... Loss: 0.014207... Val Loss: 0.133864\n",
      "Epoch: 2/4... Step: 3700... Loss: 0.008923... Val Loss: 0.132424\n",
      "Epoch: 2/4... Step: 3800... Loss: 0.000286... Val Loss: 0.147308\n",
      "Epoch: 2/4... Step: 3900... Loss: 0.117942... Val Loss: 0.143194\n",
      "Epoch: 2/4... Step: 4000... Loss: 0.015314... Val Loss: 0.137587\n",
      "Epoch: 2/4... Step: 4100... Loss: 0.006294... Val Loss: 0.138405\n",
      "Epoch: 2/4... Step: 4200... Loss: 0.065078... Val Loss: 0.142644\n",
      "Epoch: 2/4... Step: 4300... Loss: 0.008646... Val Loss: 0.131893\n",
      "Epoch: 2/4... Step: 4400... Loss: 0.183227... Val Loss: 0.133700\n",
      "Epoch: 2/4... Step: 4500... Loss: 0.070108... Val Loss: 0.132822\n",
      "Epoch: 2/4... Step: 4600... Loss: 0.071729... Val Loss: 0.131659\n",
      "Epoch: 2/4... Step: 4700... Loss: 0.008980... Val Loss: 0.135063\n",
      "Epoch: 2/4... Step: 4800... Loss: 0.011845... Val Loss: 0.141375\n",
      "Epoch: 2/4... Step: 4900... Loss: 0.057028... Val Loss: 0.133906\n",
      "Epoch: 2/4... Step: 5000... Loss: 0.139102... Val Loss: 0.131838\n",
      "Epoch: 2/4... Step: 5100... Loss: 0.094804... Val Loss: 0.133871\n",
      "Epoch: 2/4... Step: 5200... Loss: 0.017292... Val Loss: 0.136663\n",
      "Epoch: 2/4... Step: 5300... Loss: 0.047020... Val Loss: 0.136127\n",
      "Epoch: 2/4... Step: 5400... Loss: 0.209670... Val Loss: 0.133662\n",
      "Epoch: 2/4... Step: 5500... Loss: 0.103636... Val Loss: 0.132157\n",
      "Epoch: 2/4... Step: 5600... Loss: 0.078695... Val Loss: 0.129620\n",
      "Epoch: 2/4... Step: 5700... Loss: 0.326726... Val Loss: 0.131801\n",
      "Epoch: 2/4... Step: 5800... Loss: 0.087384... Val Loss: 0.130402\n",
      "Epoch: 2/4... Step: 5900... Loss: 0.269169... Val Loss: 0.131943\n",
      "Epoch: 3/4... Step: 6000... Loss: 0.010974... Val Loss: 0.138321\n",
      "Epoch: 3/4... Step: 6100... Loss: 0.006370... Val Loss: 0.149211\n",
      "Epoch: 3/4... Step: 6200... Loss: 0.024762... Val Loss: 0.141919\n",
      "Epoch: 3/4... Step: 6300... Loss: 0.038013... Val Loss: 0.149547\n",
      "Epoch: 3/4... Step: 6400... Loss: 0.007415... Val Loss: 0.161342\n",
      "Epoch: 3/4... Step: 6500... Loss: 0.161437... Val Loss: 0.163881\n",
      "Epoch: 3/4... Step: 6600... Loss: 0.014808... Val Loss: 0.171171\n",
      "Epoch: 3/4... Step: 6700... Loss: 0.104119... Val Loss: 0.166713\n",
      "Epoch: 3/4... Step: 6800... Loss: 0.019746... Val Loss: 0.154155\n",
      "Epoch: 3/4... Step: 6900... Loss: 0.000923... Val Loss: 0.164233\n",
      "Epoch: 3/4... Step: 7000... Loss: 0.085875... Val Loss: 0.169367\n",
      "Epoch: 3/4... Step: 7100... Loss: 0.030833... Val Loss: 0.170347\n",
      "Epoch: 3/4... Step: 7200... Loss: 0.011457... Val Loss: 0.172429\n",
      "Epoch: 3/4... Step: 7300... Loss: 0.057714... Val Loss: 0.161435\n",
      "Epoch: 3/4... Step: 7400... Loss: 0.013613... Val Loss: 0.160267\n",
      "Epoch: 3/4... Step: 7500... Loss: 0.022390... Val Loss: 0.166604\n",
      "Epoch: 3/4... Step: 7600... Loss: 0.153902... Val Loss: 0.183958\n",
      "Epoch: 3/4... Step: 7700... Loss: 0.023674... Val Loss: 0.172214\n",
      "Epoch: 3/4... Step: 7800... Loss: 0.002516... Val Loss: 0.166851\n",
      "Epoch: 3/4... Step: 7900... Loss: 0.005976... Val Loss: 0.171250\n",
      "Epoch: 3/4... Step: 8000... Loss: 0.006802... Val Loss: 0.174778\n",
      "Epoch: 3/4... Step: 8100... Loss: 0.006669... Val Loss: 0.172435\n",
      "Epoch: 3/4... Step: 8200... Loss: 0.154375... Val Loss: 0.172820\n",
      "Epoch: 3/4... Step: 8300... Loss: 0.010360... Val Loss: 0.173303\n",
      "Epoch: 3/4... Step: 8400... Loss: 0.049624... Val Loss: 0.164571\n",
      "Epoch: 3/4... Step: 8500... Loss: 0.020542... Val Loss: 0.175265\n",
      "Epoch: 3/4... Step: 8600... Loss: 0.239622... Val Loss: 0.169789\n",
      "Epoch: 3/4... Step: 8700... Loss: 0.057685... Val Loss: 0.172496\n",
      "Epoch: 3/4... Step: 8800... Loss: 0.025239... Val Loss: 0.162584\n",
      "Epoch: 3/4... Step: 8900... Loss: 0.004198... Val Loss: 0.165919\n",
      "Epoch: 4/4... Step: 9000... Loss: 0.008437... Val Loss: 0.179659\n",
      "Epoch: 4/4... Step: 9100... Loss: 0.016185... Val Loss: 0.186820\n",
      "Epoch: 4/4... Step: 9200... Loss: 0.005775... Val Loss: 0.193700\n",
      "Epoch: 4/4... Step: 9300... Loss: 0.036900... Val Loss: 0.195586\n",
      "Epoch: 4/4... Step: 9400... Loss: 0.083759... Val Loss: 0.196187\n",
      "Epoch: 4/4... Step: 9500... Loss: 0.000194... Val Loss: 0.204881\n",
      "Epoch: 4/4... Step: 9600... Loss: 0.012633... Val Loss: 0.202530\n",
      "Epoch: 4/4... Step: 9700... Loss: 0.007425... Val Loss: 0.227473\n",
      "Epoch: 4/4... Step: 9800... Loss: 0.004506... Val Loss: 0.205171\n",
      "Epoch: 4/4... Step: 9900... Loss: 0.003334... Val Loss: 0.218026\n",
      "Epoch: 4/4... Step: 10000... Loss: 0.090910... Val Loss: 0.217093\n",
      "Epoch: 4/4... Step: 10100... Loss: 0.002377... Val Loss: 0.209331\n",
      "Epoch: 4/4... Step: 10200... Loss: 0.007761... Val Loss: 0.223303\n",
      "Epoch: 4/4... Step: 10300... Loss: 0.031320... Val Loss: 0.211929\n",
      "Epoch: 4/4... Step: 10400... Loss: 0.006215... Val Loss: 0.213494\n",
      "Epoch: 4/4... Step: 10500... Loss: 0.004334... Val Loss: 0.211681\n",
      "Epoch: 4/4... Step: 10600... Loss: 0.025860... Val Loss: 0.216631\n",
      "Epoch: 4/4... Step: 10700... Loss: 0.019631... Val Loss: 0.223892\n",
      "Epoch: 4/4... Step: 10800... Loss: 0.030947... Val Loss: 0.229945\n",
      "Epoch: 4/4... Step: 10900... Loss: 0.000295... Val Loss: 0.219920\n",
      "Epoch: 4/4... Step: 11000... Loss: 0.021982... Val Loss: 0.217523\n",
      "Epoch: 4/4... Step: 11100... Loss: 0.015100... Val Loss: 0.218964\n",
      "Epoch: 4/4... Step: 11200... Loss: 0.003069... Val Loss: 0.208398\n",
      "Epoch: 4/4... Step: 11300... Loss: 0.005021... Val Loss: 0.213429\n",
      "Epoch: 4/4... Step: 11400... Loss: 0.001288... Val Loss: 0.220511\n",
      "Epoch: 4/4... Step: 11500... Loss: 0.000669... Val Loss: 0.198440\n",
      "Epoch: 4/4... Step: 11600... Loss: 0.132004... Val Loss: 0.203614\n",
      "Epoch: 4/4... Step: 11700... Loss: 0.006241... Val Loss: 0.206320\n",
      "Epoch: 4/4... Step: 11800... Loss: 0.065141... Val Loss: 0.214962\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, None)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, None)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
