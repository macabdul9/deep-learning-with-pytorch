{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import torch\n",
    "\n",
    "# data\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# training and evaluation\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 120\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create source and target field\n",
    "source = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    lower=True,\n",
    "    batch_first=True,    \n",
    ")\n",
    "\n",
    "target = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en\",\n",
    "    lower=True,\n",
    "    batch_first=True,    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train, test, val = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(source, target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary\n",
    "source.build_vocab(train, min_freq=2)\n",
    "target.build_vocab(train, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_loader, test_loader, val_loader = BucketIterator.splits(\n",
    "    datasets=(train, test, val),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 32]) torch.Size([128, 29])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.src.size(), batch.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout=0.20, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.seq = nn.GRU(input_size=embedding_dim, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "    \n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src.size() -> [batch, seq_len]\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, hidden = self.seq(embedded)\n",
    "        return hidden, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![seq2seq](./seq2seq7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, dropout=0.20, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.seq = nn.GRU(\n",
    "            input_size=embedding_dim + hidden_size, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "#             batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(in_features=2*hidden_size+embedding_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, context):\n",
    "        \n",
    "        # a single token will be fed to decoder \n",
    "        # initially hidden vector and context vector will be same\n",
    "        \"\"\"\n",
    "            input.size()   -> [1, batch]\n",
    "            hidden.size()  -> [batch, 1, hidden_size]\n",
    "            context.size() -> [batch, 1,  hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.embedding(input)\n",
    "        # embedded.size() -> [batch_size, emb_dim]\n",
    "        \n",
    "        embedded_context = torch.cat((embedded.unsqueeze(0), context.type_as(embedded)), dim=2)\n",
    "        # embedded_context.size() -> [1, batch_size, emb_dim+hidden_size]\n",
    "    \n",
    "        outputs, hidden = self.seq(embedded_context, hidden)\n",
    "        # both outputs and hidden will be same\n",
    "        \n",
    "\n",
    "        # output, embedding and conext vector\n",
    "        combined = torch.cat((outputs, context, embedded.unsqueeze(0)), dim=2)\n",
    "        \n",
    "        predictions  = F.softmax(self.fc(combined.squeeze()), dim=1)\n",
    "        \n",
    "        return predictions, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        \"\"\"\n",
    "            src.size() -> [batch, seq_len]\n",
    "            trg.size() -> [batch, seq_len]\n",
    "        \"\"\"\n",
    "        batch = trg.size(0)\n",
    "        seq_len = trg.size(1)\n",
    "        vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        hidden, context = self.encoder(src)\n",
    "        \n",
    "        # create an empty tensor to store the outputs\n",
    "        # outputs.shape -> [batch_size, seq_len, vocab_size]\n",
    "        outputs = torch.empty((batch, seq_len, vocab_size))\n",
    "        \n",
    "        # take first token of each samples in the batch\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            predictions, hidden = self.decoder(input, hidden, context)\n",
    "            input = trg[:, i]\n",
    "            outputs[:, i-1, :] = predictions\n",
    "        return outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data, criterion):\n",
    "    loss, ppl = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data:\n",
    "            outputs = model(batch.src, batch.trg)\n",
    "            bs, seq_len, vocab_size = outputs.size(0), outputs.size(1), outputs.size(2)\n",
    "            l = criterion(outputs.view(bs*seq_len, vocab_size), batch.trg.view(-1))\n",
    "            p = torch.exp(l)\n",
    "            loss.append(l.item())\n",
    "            ppl.append(p.item())\n",
    "    return sum(loss)/len(loss), sum(ppl)/len(ppl)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model params\n",
    "embedding_dim = 100\n",
    "hidden_size = 64\n",
    "\n",
    "# training configuration\n",
    "epochs = 10\n",
    "lr = 0.1\n",
    "PAD_IDX = target.vocab[target.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the models\n",
    "encoder = Encoder(vocab_size=len(source.vocab), embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)\n",
    "decoder = Decoder(vocab_size=len(source.vocab), embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)\n",
    "model = Seq2Seq(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer and criterion\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0 | Steps 0\n",
      "train_loss 8.9687 | train_ppl 7853.5610\n",
      "val_loss 8.3490 | val_ppl 4226.8874\n",
      "Epochs 0 | Steps 200\n",
      "train_loss 8.1844 | train_ppl 3584.6763\n",
      "val_loss 8.1785 | val_ppl 3563.7547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 1/10 [10:19<1:32:52, 619.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 1 | Steps 400\n",
      "train_loss 8.1833 | train_ppl 3580.7642\n",
      "val_loss 8.1785 | val_ppl 3563.6953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 2/10 [17:17<1:14:31, 558.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 2 | Steps 600\n",
      "train_loss 8.2058 | train_ppl 3662.2341\n",
      "val_loss 8.1784 | val_ppl 3563.5409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 3/10 [21:45<55:01, 471.70s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 3 | Steps 800\n",
      "train_loss 8.1762 | train_ppl 3555.1658\n",
      "val_loss 8.1784 | val_ppl 3563.5422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 4/10 [25:41<40:06, 401.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 4 | Steps 1000\n",
      "train_loss 8.2028 | train_ppl 3651.2561\n",
      "val_loss 8.1784 | val_ppl 3563.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 5/10 [29:35<29:14, 350.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 5 | Steps 1200\n",
      "train_loss 8.1843 | train_ppl 3584.3447\n",
      "val_loss 8.1784 | val_ppl 3563.5443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 6/10 [33:43<21:19, 319.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 6 | Steps 1400\n",
      "train_loss 8.1880 | train_ppl 3597.5122\n",
      "val_loss 8.1882 | val_ppl 3598.8373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 7/10 [38:00<15:03, 301.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 7 | Steps 1600\n",
      "train_loss 8.2086 | train_ppl 3672.5132\n",
      "val_loss 8.1883 | val_ppl 3599.5074\n"
     ]
    }
   ],
   "source": [
    "epoch_progress = tqdm.tqdm(total=epochs, desc=\"Epoch\", position=0)\n",
    "steps = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss, epoch_ppl = [], []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(batch.src, batch.trg)\n",
    "        bs, seq_len =  outputs.size(0), outputs.size(1)\n",
    "        \n",
    "        # calculate loss and backpropagate the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs.view(bs*seq_len, -1), batch.trg.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if steps % 200 == 0:\n",
    "            val_loss, val_ppl = eval(model, val_loader, criterion)\n",
    "            print(f'Epochs {epoch} | Steps {steps}')\n",
    "            print(f'train_loss {loss.item():.4f} | train_ppl {torch.exp(loss).item():.4f}')\n",
    "            print(f'val_loss {val_loss:.4f} | val_ppl {val_ppl:.4f}')\n",
    "        steps += 1\n",
    "    epoch_progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
