{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accuracy import accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"./data\"\n",
    "log_dir = \"./runs\"\n",
    "input_size = 28*28\n",
    "out_size = 10\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "lr = 1e-3\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "val = MNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=6,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size=28*28, n_classes=10):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_size, out_features=n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(input_size=input_size, n_classes=out_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, Optimizer and Tensorboard writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer, loss and tensorboard writer function\n",
    "optimizer = Adam(params=model.parameters(), lr=lr)\n",
    "criterian = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0 |  step  1 | train_loss 2.307408094406128 | train_acc 6.25\n",
      "epoch  0 |  step  501 | train_loss 0.39344289898872375 | train_acc 85.9375\n",
      "end of epoch 0 | train_loss 0.543136858673238 | train_acc 86.75039978678038 | val_loss 0.3460503515640895 | val_acc 90.58333333333333\n",
      "epoch  1 |  step  939 | train_loss 0.4421619772911072 | train_acc 85.9375\n",
      "epoch  1 |  step  1439 | train_loss 0.1482522338628769 | train_acc 100.0\n",
      "end of epoch 1 | train_loss 0.3235077039916505 | train_acc 91.03144989339019 | val_loss 0.3024573971649011 | val_acc 91.62333333333333\n",
      "epoch  2 |  step  1877 | train_loss 0.23878110945224762 | train_acc 93.75\n",
      "epoch  2 |  step  2377 | train_loss 0.3309534788131714 | train_acc 92.1875\n",
      "end of epoch 2 | train_loss 0.2953710081035903 | train_acc 91.74606876332622 | val_loss 0.2838791015545527 | val_acc 92.115\n",
      "epoch  3 |  step  2815 | train_loss 0.3046310245990753 | train_acc 93.75\n",
      "epoch  3 |  step  3315 | train_loss 0.41535767912864685 | train_acc 85.9375\n",
      "end of epoch 3 | train_loss 0.2817848443921441 | train_acc 92.08255597014926 | val_loss 0.27212033713062606 | val_acc 92.44333333333333\n",
      "epoch  4 |  step  3753 | train_loss 0.28455162048339844 | train_acc 92.1875\n",
      "epoch  4 |  step  4253 | train_loss 0.25667881965637207 | train_acc 90.625\n",
      "end of epoch 4 | train_loss 0.2738628893065999 | train_acc 92.3790644989339 | val_loss 0.2658118845840295 | val_acc 92.59166666666667\n",
      "epoch  5 |  step  4691 | train_loss 0.2732425928115845 | train_acc 92.1875\n",
      "epoch  5 |  step  5191 | train_loss 0.30225643515586853 | train_acc 93.75\n",
      "end of epoch 5 | train_loss 0.2684105101154684 | train_acc 92.46735074626865 | val_loss 0.26136296308736007 | val_acc 92.72166666666666\n",
      "epoch  6 |  step  5629 | train_loss 0.28918948769569397 | train_acc 92.1875\n",
      "epoch  6 |  step  6129 | train_loss 0.37083175778388977 | train_acc 90.625\n",
      "end of epoch 6 | train_loss 0.2637420226412732 | train_acc 92.68556769722815 | val_loss 0.2567961567759514 | val_acc 92.84\n",
      "epoch  7 |  step  6567 | train_loss 0.1854628324508667 | train_acc 93.75\n",
      "epoch  7 |  step  7067 | train_loss 0.5576097965240479 | train_acc 87.5\n",
      "end of epoch 7 | train_loss 0.2603058803842457 | train_acc 92.7621934968017 | val_loss 0.2525105551292499 | val_acc 92.96833333333333\n",
      "epoch  8 |  step  7505 | train_loss 0.3274998962879181 | train_acc 92.1875\n",
      "epoch  8 |  step  8005 | train_loss 0.17655979096889496 | train_acc 95.3125\n",
      "end of epoch 8 | train_loss 0.2568885648905087 | train_acc 92.89045842217485 | val_loss 0.25216841523746647 | val_acc 93.14833333333333\n",
      "epoch  9 |  step  8443 | train_loss 0.13632261753082275 | train_acc 95.3125\n",
      "epoch  9 |  step  8943 | train_loss 0.13182951509952545 | train_acc 96.875\n",
      "end of epoch 9 | train_loss 0.2548121961949667 | train_acc 93.00373134328358 | val_loss 0.2484389713148276 | val_acc 93.16833333333334\n",
      "epoch  10 |  step  9381 | train_loss 0.25364336371421814 | train_acc 93.75\n",
      "epoch  10 |  step  9881 | train_loss 0.17477285861968994 | train_acc 96.875\n",
      "end of epoch 10 | train_loss 0.2525845128002324 | train_acc 92.98707356076758 | val_loss 0.25083122042566536 | val_acc 93.07\n",
      "epoch  11 |  step  10319 | train_loss 0.16561125218868256 | train_acc 95.3125\n",
      "epoch  11 |  step  10819 | train_loss 0.26523515582084656 | train_acc 92.1875\n",
      "end of epoch 11 | train_loss 0.2512922235516343 | train_acc 93.05703624733475 | val_loss 0.24420213469068208 | val_acc 93.26\n",
      "epoch  12 |  step  11257 | train_loss 0.18477624654769897 | train_acc 96.875\n",
      "epoch  12 |  step  11757 | train_loss 0.21356280148029327 | train_acc 93.75\n",
      "end of epoch 12 | train_loss 0.24950600793954533 | train_acc 93.12533315565032 | val_loss 0.24314320149719715 | val_acc 93.33666666666667\n",
      "epoch  13 |  step  12195 | train_loss 0.21908719837665558 | train_acc 95.3125\n",
      "epoch  13 |  step  12695 | train_loss 0.10256421566009521 | train_acc 98.4375\n",
      "end of epoch 13 | train_loss 0.24799110138340036 | train_acc 93.13865938166312 | val_loss 0.24310760017931463 | val_acc 93.40333333333334\n",
      "epoch  14 |  step  13133 | train_loss 0.14165177941322327 | train_acc 95.3125\n",
      "epoch  14 |  step  13633 | train_loss 0.16487739980220795 | train_acc 96.875\n",
      "end of epoch 14 | train_loss 0.24668858243998434 | train_acc 93.1786380597015 | val_loss 0.24088561014533044 | val_acc 93.40833333333333\n",
      "epoch  15 |  step  14071 | train_loss 0.28836867213249207 | train_acc 87.5\n",
      "epoch  15 |  step  14571 | train_loss 0.28087589144706726 | train_acc 90.625\n",
      "end of epoch 15 | train_loss 0.245642733226008 | train_acc 93.23860607675905 | val_loss 0.24108644756476083 | val_acc 93.35166666666667\n",
      "epoch  16 |  step  15009 | train_loss 0.1786704957485199 | train_acc 96.875\n",
      "epoch  16 |  step  15509 | train_loss 0.22759884595870972 | train_acc 90.625\n",
      "end of epoch 16 | train_loss 0.2441392115025378 | train_acc 93.29857409381663 | val_loss 0.23962499518096447 | val_acc 93.35333333333334\n",
      "epoch  17 |  step  15947 | train_loss 0.448078453540802 | train_acc 90.625\n",
      "epoch  17 |  step  16447 | train_loss 0.1817234307527542 | train_acc 93.75\n",
      "end of epoch 17 | train_loss 0.24347968875313364 | train_acc 93.27192164179104 | val_loss 0.2364308224827051 | val_acc 93.60166666666667\n",
      "epoch  18 |  step  16885 | train_loss 0.274557501077652 | train_acc 93.75\n",
      "epoch  18 |  step  17385 | train_loss 0.5213901996612549 | train_acc 87.5\n",
      "end of epoch 18 | train_loss 0.24256594075577093 | train_acc 93.33522121535181 | val_loss 0.2364039967507124 | val_acc 93.51166666666667\n",
      "epoch  19 |  step  17823 | train_loss 0.20402179658412933 | train_acc 95.3125\n",
      "epoch  19 |  step  18323 | train_loss 0.17909309267997742 | train_acc 95.3125\n",
      "end of epoch 19 | train_loss 0.24196698597626393 | train_acc 93.29524253731343 | val_loss 0.23607879281938077 | val_acc 93.495\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "epochs_acc = []\n",
    "steps = 0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    epoch_loss_train = []\n",
    "    epoch_acc_train = []\n",
    "    epoch_loss_val = []\n",
    "    epoch_acc_val = []\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        images, labels = batch\n",
    "        images = images.view(-1, 28*28)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # determine the loss and acc\n",
    "        loss = criterian(outputs, labels)\n",
    "        acc = accuracy(model=model, data=batch, loader=False)\n",
    "        \n",
    "        # backpropagate the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # append the step loss and step acc\n",
    "        epoch_loss_train.append(loss.item())\n",
    "        epoch_acc_train.append(acc)\n",
    "        \n",
    "        if i%500==0:\n",
    "            print(f'epoch  {epoch} |  step  {steps} | train_loss {loss.item()} | train_acc {acc}')\n",
    "            writer.add_scalar(\"step-wise training loss\", loss.item(), steps)\n",
    "            writer.add_scalar(\"step-wise training acc\", acc, steps)\n",
    "        \n",
    "    for i, batch in enumerate(val_loader):\n",
    "        with torch.no_grad():\n",
    "            images, labels = batch\n",
    "            images = images.view(-1, 28*28)\n",
    "            outputs = model(images)\n",
    "\n",
    "            # determine the loss and acc\n",
    "            loss = criterian(outputs, labels)\n",
    "            acc = accuracy(model=model, data=batch, loader=False)\n",
    "\n",
    "            # append the step loss and step acc\n",
    "            epoch_loss_val.append(loss.item())\n",
    "            epoch_acc_val.append(acc)\n",
    "            \n",
    "    t_loss = sum(epoch_loss_train)/len(epoch_loss_train)\n",
    "    t_acc = sum(epoch_acc_train)/len(epoch_acc_train)\n",
    "    \n",
    "    v_loss = sum(epoch_loss_val)/len(epoch_loss_val)\n",
    "    v_acc = sum(epoch_acc_val)/len(epoch_acc_val)\n",
    "    \n",
    "    writer.add_scalar(\"training loss \", t_loss, epoch)\n",
    "    writer.add_scalar(\"validation loss\", v_loss, epoch)\n",
    "    writer.add_scalar(\"training acc\", t_acc, epoch)\n",
    "    writer.add_scalar(\"validation acc\", v_acc, epoch)\n",
    "    \n",
    "\n",
    "    print(f'end of epoch {epoch} | train_loss {t_loss} | train_acc {t_acc} | val_loss {v_loss} | val_acc {v_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.2.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With PyTorch-Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(pl.LightningModule):\n",
    "    \n",
    "    \n",
    "    def __init__(self, in_features = 28*28, n_classes = 10):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=in_features, out_features=n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x.view(-1, 28*28))\n",
    "        return out\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return Adam(self.parameters(), lr=lr)\n",
    "    \n",
    "    def accuracy(self, y, y_hat):\n",
    "        correct = (y==torch.argmax(y_hat, 1)).sum().item()\n",
    "        total = y.size(0)\n",
    "        return correct/total\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        dataset = MNIST(root=root, train=True, transform=transform, download=True)\n",
    "        loader = DataLoader(dataset=dataset, shuffle=True, batch_size=batch_size, num_workers=6)\n",
    "        return loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        dataset = MNIST(root=root, train=False, transform=transform, download=True)\n",
    "        loader = DataLoader(dataset=dataset, shuffle=False, batch_size=batch_size, num_workers=6)\n",
    "        return loader\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "#         acc = self.accuracy(y=y, y_hat=y_hat)\n",
    "        tensorboard_logs = {\"loss\":loss}\n",
    "        return {\"loss\":loss, \"log\":tensorboard_logs}\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "#         avg_acc = torch.stack([x['train_acc'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"epoch_train_loss\":avg_loss}\n",
    "        return {\"epoch_train_loss\":avg_loss, \"log\":tensorboard_logs}\n",
    "        \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "#         acc = self.accuracy(y=y, y_hat=y_hat)\n",
    "        tensorboard_logs = {\"val_loss\":loss}\n",
    "        return {\"val_loss\":loss, \"log\":tensorboard_logs}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "#         avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        tensorboard_logs = {\"epoch_val_loss\":avg_loss}\n",
    "        return {\"epoch_val_loss\":avg_loss, \"log\":tensorboard_logs}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(gpus=[0], max_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name   | Type   | Params\n",
      "------------------------------\n",
      "0 | linear | Linear | 7 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  86%|████████▌ | 938/1095 [00:03<00:00, 252.13it/s, loss=0.371, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1:  88%|████████▊ | 964/1095 [00:03<00:00, 247.33it/s, loss=0.371, v_num=0]\n",
      "Epoch 1:  95%|█████████▍| 1036/1095 [00:03<00:00, 259.10it/s, loss=0.371, v_num=0]\n",
      "Epoch 1: 100%|██████████| 1095/1095 [00:04<00:00, 266.29it/s, loss=0.371, v_num=0]\n",
      "Epoch 2:   0%|          | 0/1095 [00:00<?, ?it/s, loss=0.371, v_num=0]            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: RuntimeWarning: The metric you returned None must be a Torch.Tensor instance, checkpoint not saved HINT: what is the value of val_loss in validation_end()?\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:23: RuntimeWarning: Can save best model only with val_loss available, skipping.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:  86%|████████▌ | 938/1095 [00:03<00:00, 288.87it/s, loss=0.284, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/157 [00:00<00:30,  5.12it/s]\u001b[A\n",
      "Epoch 2:  92%|█████████▏| 1008/1095 [00:03<00:00, 283.47it/s, loss=0.284, v_num=0]\n",
      "Epoch 2: 100%|██████████| 1095/1095 [00:03<00:00, 293.18it/s, loss=0.284, v_num=0]\n",
      "Epoch 3:  86%|████████▌ | 938/1095 [00:03<00:00, 298.33it/s, loss=0.334, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/157 [00:00<00:23,  6.77it/s]\u001b[A\n",
      "Epoch 3:  92%|█████████▏| 1008/1095 [00:03<00:00, 296.51it/s, loss=0.334, v_num=0]\n",
      "Epoch 3: 100%|██████████| 1095/1095 [00:03<00:00, 308.33it/s, loss=0.334, v_num=0]\n",
      "Epoch 4:  86%|████████▌ | 938/1095 [00:03<00:00, 311.88it/s, loss=0.297, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   1%|          | 1/157 [00:00<00:23,  6.59it/s]\u001b[A\n",
      "Epoch 4:  92%|█████████▏| 1008/1095 [00:03<00:00, 308.59it/s, loss=0.297, v_num=0]\n",
      "Epoch 4: 100%|██████████| 1095/1095 [00:03<00:00, 318.90it/s, loss=0.297, v_num=0]\n",
      "Epoch 5:  86%|████████▌ | 938/1095 [00:03<00:00, 304.80it/s, loss=0.283, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5:  88%|████████▊ | 962/1095 [00:03<00:00, 295.77it/s, loss=0.283, v_num=0]\n",
      "Epoch 5:  95%|█████████▍| 1036/1095 [00:03<00:00, 308.71it/s, loss=0.283, v_num=0]\n",
      "Epoch 5: 100%|██████████| 1095/1095 [00:03<00:00, 314.74it/s, loss=0.283, v_num=0]\n",
      "Epoch 6:  86%|████████▌ | 938/1095 [00:03<00:00, 302.14it/s, loss=0.344, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6:  88%|████████▊ | 962/1095 [00:03<00:00, 293.35it/s, loss=0.344, v_num=0]\n",
      "Epoch 6:  95%|█████████▍| 1036/1095 [00:03<00:00, 305.36it/s, loss=0.344, v_num=0]\n",
      "Epoch 6: 100%|██████████| 1095/1095 [00:03<00:00, 309.29it/s, loss=0.344, v_num=0]\n",
      "Epoch 7:  86%|████████▌ | 938/1095 [00:02<00:00, 312.75it/s, loss=0.304, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7:  88%|████████▊ | 962/1095 [00:03<00:00, 302.49it/s, loss=0.304, v_num=0]\n",
      "Epoch 7:  95%|█████████▍| 1036/1095 [00:03<00:00, 315.22it/s, loss=0.304, v_num=0]\n",
      "Epoch 7: 100%|██████████| 1095/1095 [00:03<00:00, 321.72it/s, loss=0.304, v_num=0]\n",
      "Epoch 8:  86%|████████▌ | 938/1095 [00:03<00:00, 288.78it/s, loss=0.302, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8:  88%|████████▊ | 962/1095 [00:03<00:00, 280.53it/s, loss=0.302, v_num=0]\n",
      "Epoch 8:  95%|█████████▍| 1036/1095 [00:03<00:00, 291.10it/s, loss=0.302, v_num=0]\n",
      "Epoch 8: 100%|██████████| 1095/1095 [00:03<00:00, 297.48it/s, loss=0.302, v_num=0]\n",
      "Epoch 9:  86%|████████▌ | 938/1095 [00:03<00:00, 291.02it/s, loss=0.277, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9:  88%|████████▊ | 962/1095 [00:03<00:00, 281.74it/s, loss=0.277, v_num=0]\n",
      "Epoch 9:  95%|█████████▍| 1036/1095 [00:03<00:00, 292.81it/s, loss=0.277, v_num=0]\n",
      "Epoch 9: 100%|██████████| 1095/1095 [00:03<00:00, 292.62it/s, loss=0.277, v_num=0]\n",
      "Epoch 10:  86%|████████▌ | 938/1095 [00:03<00:00, 285.40it/s, loss=0.283, v_num=0]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10:  88%|████████▊ | 962/1095 [00:03<00:00, 277.35it/s, loss=0.283, v_num=0]\n",
      "Epoch 10:  95%|█████████▍| 1036/1095 [00:03<00:00, 289.63it/s, loss=0.283, v_num=0]\n",
      "Epoch 10: 100%|██████████| 1095/1095 [00:03<00:00, 295.98it/s, loss=0.283, v_num=0]\n",
      "Epoch 11:  86%|████████▌ | 938/1095 [00:03<00:00, 298.04it/s, loss=0.228, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11:  88%|████████▊ | 962/1095 [00:03<00:00, 289.11it/s, loss=0.228, v_num=0]\n",
      "Epoch 11:  95%|█████████▍| 1036/1095 [00:03<00:00, 299.97it/s, loss=0.228, v_num=0]\n",
      "Epoch 11: 100%|██████████| 1095/1095 [00:03<00:00, 304.07it/s, loss=0.228, v_num=0]\n",
      "Epoch 12:  86%|████████▌ | 938/1095 [00:03<00:00, 305.58it/s, loss=0.235, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12:  88%|████████▊ | 962/1095 [00:03<00:00, 295.83it/s, loss=0.235, v_num=0]\n",
      "Epoch 12:  95%|█████████▍| 1036/1095 [00:03<00:00, 307.67it/s, loss=0.235, v_num=0]\n",
      "Epoch 12: 100%|██████████| 1095/1095 [00:03<00:00, 313.34it/s, loss=0.235, v_num=0]\n",
      "Epoch 13:  86%|████████▌ | 938/1095 [00:03<00:00, 307.65it/s, loss=0.255, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13:  88%|████████▊ | 962/1095 [00:03<00:00, 297.92it/s, loss=0.255, v_num=0]\n",
      "Epoch 13:  95%|█████████▍| 1036/1095 [00:03<00:00, 308.09it/s, loss=0.255, v_num=0]\n",
      "Epoch 13: 100%|██████████| 1095/1095 [00:03<00:00, 313.49it/s, loss=0.255, v_num=0]\n",
      "Epoch 14:  86%|████████▌ | 938/1095 [00:03<00:00, 301.66it/s, loss=0.271, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14:  88%|████████▊ | 962/1095 [00:03<00:00, 292.27it/s, loss=0.271, v_num=0]\n",
      "Epoch 14:  95%|█████████▍| 1036/1095 [00:03<00:00, 304.19it/s, loss=0.271, v_num=0]\n",
      "Epoch 14: 100%|██████████| 1095/1095 [00:03<00:00, 309.84it/s, loss=0.271, v_num=0]\n",
      "Epoch 15:  86%|████████▌ | 938/1095 [00:03<00:00, 280.73it/s, loss=0.250, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15:  88%|████████▊ | 962/1095 [00:03<00:00, 270.64it/s, loss=0.250, v_num=0]\n",
      "Epoch 15:  95%|█████████▍| 1036/1095 [00:03<00:00, 281.86it/s, loss=0.250, v_num=0]\n",
      "Epoch 15: 100%|██████████| 1095/1095 [00:03<00:00, 286.92it/s, loss=0.250, v_num=0]\n",
      "Epoch 16:  86%|████████▌ | 938/1095 [00:03<00:00, 290.08it/s, loss=0.264, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16:  88%|████████▊ | 962/1095 [00:03<00:00, 281.46it/s, loss=0.264, v_num=0]\n",
      "Epoch 16:  95%|█████████▍| 1036/1095 [00:03<00:00, 291.05it/s, loss=0.264, v_num=0]\n",
      "Epoch 16: 100%|██████████| 1095/1095 [00:03<00:00, 297.63it/s, loss=0.264, v_num=0]\n",
      "Epoch 17:  86%|████████▌ | 938/1095 [00:03<00:00, 299.88it/s, loss=0.211, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17:  88%|████████▊ | 962/1095 [00:03<00:00, 290.40it/s, loss=0.211, v_num=0]\n",
      "Epoch 17:  95%|█████████▍| 1036/1095 [00:03<00:00, 302.53it/s, loss=0.211, v_num=0]\n",
      "Epoch 17: 100%|██████████| 1095/1095 [00:03<00:00, 307.65it/s, loss=0.211, v_num=0]\n",
      "Epoch 18:  86%|████████▌ | 938/1095 [00:03<00:00, 301.15it/s, loss=0.257, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18:  88%|████████▊ | 962/1095 [00:03<00:00, 291.49it/s, loss=0.257, v_num=0]\n",
      "Epoch 18:  95%|█████████▍| 1036/1095 [00:03<00:00, 303.65it/s, loss=0.257, v_num=0]\n",
      "Epoch 18: 100%|██████████| 1095/1095 [00:03<00:00, 308.50it/s, loss=0.257, v_num=0]\n",
      "Epoch 19:  86%|████████▌ | 938/1095 [00:03<00:00, 302.67it/s, loss=0.281, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19:  88%|████████▊ | 962/1095 [00:03<00:00, 293.16it/s, loss=0.281, v_num=0]\n",
      "Epoch 19:  95%|█████████▍| 1036/1095 [00:03<00:00, 305.25it/s, loss=0.281, v_num=0]\n",
      "Epoch 19: 100%|██████████| 1095/1095 [00:03<00:00, 310.57it/s, loss=0.281, v_num=0]\n",
      "Epoch 20:  86%|████████▌ | 938/1095 [00:03<00:00, 306.73it/s, loss=0.265, v_num=0] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20:  88%|████████▊ | 962/1095 [00:03<00:00, 296.94it/s, loss=0.265, v_num=0]\n",
      "Epoch 20:  95%|█████████▍| 1036/1095 [00:03<00:00, 308.67it/s, loss=0.265, v_num=0]\n",
      "Epoch 20: 100%|██████████| 1095/1095 [00:03<00:00, 312.77it/s, loss=0.265, v_num=0]\n",
      "Epoch 20: 100%|██████████| 1095/1095 [00:03<00:00, 312.29it/s, loss=0.265, v_num=0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
