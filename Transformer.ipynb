{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from utils import count_parameters\n",
    "import torch\n",
    "\n",
    "# data\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# training\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data fields for source and target\n",
    "source = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True\n",
    ")\n",
    "target = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the parallel corpus\n",
    "train, val, test = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(source, target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "source.build_vocab(train)\n",
    "target.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader, val_laoder, test_loader = BucketIterator.splits(\n",
    "    datasets=(train, val, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 29]) torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "batch =  next(iter(train_loader))\n",
    "print(batch.src.shape, batch.trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        transformer encoder module returns a [batch_size, seq_len, out_dim] tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, n_heads, pf_dim, dropout=100, max_len=100):\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # tok and pos embedding dim is same because we have to add them\n",
    "        self.tok_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=embedding_dim)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
       "\n",
       "This module is often used to store word embeddings and retrieve them using indices.\n",
       "The input to the module is a list of indices, and the output is the corresponding\n",
       "word embeddings.\n",
       "\n",
       "Args:\n",
       "    num_embeddings (int): size of the dictionary of embeddings\n",
       "    embedding_dim (int): the size of each embedding vector\n",
       "    padding_idx (int, optional): If given, pads the output with the embedding vector at :attr:`padding_idx`\n",
       "                                     (initialized to zeros) whenever it encounters the index.\n",
       "    max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`\n",
       "                                is renormalized to have norm :attr:`max_norm`.\n",
       "    norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.\n",
       "    scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of\n",
       "                                            the words in the mini-batch. Default ``False``.\n",
       "    sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.\n",
       "                             See Notes for more details regarding sparse gradients.\n",
       "\n",
       "Attributes:\n",
       "    weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)\n",
       "                     initialized from :math:`\\mathcal{N}(0, 1)`\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(*)`, LongTensor of arbitrary shape containing the indices to extract\n",
       "    - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`\n",
       "\n",
       ".. note::\n",
       "    Keep in mind that only a limited number of optimizers support\n",
       "    sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),\n",
       "    :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)\n",
       "\n",
       ".. note::\n",
       "    With :attr:`padding_idx` set, the embedding vector at\n",
       "    :attr:`padding_idx` is initialized to all zeros. However, note that this\n",
       "    vector can be modified afterwards, e.g., using a customized\n",
       "    initialization method, and thus changing the vector used to pad the\n",
       "    output. The gradient for this vector from :class:`~torch.nn.Embedding`\n",
       "    is always zero.\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # an Embedding module containing 10 tensors of size 3\n",
       "    >>> embedding = nn.Embedding(10, 3)\n",
       "    >>> # a batch of 2 samples of 4 indices each\n",
       "    >>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])\n",
       "    >>> embedding(input)\n",
       "    tensor([[[-0.0251, -1.6902,  0.7172],\n",
       "             [-0.6431,  0.0748,  0.6969],\n",
       "             [ 1.4970,  1.3448, -0.9685],\n",
       "             [-0.3677, -2.7265, -0.1685]],\n",
       "\n",
       "            [[ 1.4970,  1.3448, -0.9685],\n",
       "             [ 0.4362, -0.4004,  0.9400],\n",
       "             [-0.6431,  0.0748,  0.6969],\n",
       "             [ 0.9124, -2.3616,  1.1151]]])\n",
       "\n",
       "\n",
       "    >>> # example with padding_idx\n",
       "    >>> embedding = nn.Embedding(10, 3, padding_idx=0)\n",
       "    >>> input = torch.LongTensor([[0,2,0,5]])\n",
       "    >>> embedding(input)\n",
       "    tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "             [ 0.1535, -2.0309,  0.9315],\n",
       "             [ 0.0000,  0.0000,  0.0000],\n",
       "             [-0.1655,  0.9897,  0.0635]]])\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.7/site-packages/torch/nn/modules/sparse.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.Embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
