{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from utils import count_parameters\n",
    "import torch\n",
    "\n",
    "# data\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# training\n",
    "import torch.optim as optim\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data fields for source and target\n",
    "source = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True\n",
    ")\n",
    "target = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the parallel corpus\n",
    "train, val, test = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(source, target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "source.build_vocab(train)\n",
    "target.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader, val_laoder, test_loader = BucketIterator.splits(\n",
    "    datasets=(train, val, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 27]) torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "batch =  next(iter(train_loader))\n",
    "print(batch.src.shape, batch.trg.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        transformer encoder module returns a [batch_size, seq_len, out_dim] tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, num_layers, n_heads, pf_dim, dropout=0.15, max_len=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # tok and pos embedding dim is same because we have to add them\n",
    "        self.tok_embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        self.pos_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=embedding_dim)\n",
    "        \n",
    "        \n",
    "        # encoder layers of transformer encoder module\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(embedding_dim, n_heads, pf_dim, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # scaling\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_dim]))\n",
    "        \n",
    "    \n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "            src.shape -> [batch, src_len]\n",
    "            src_mask -> [batch, src_len]\n",
    "        \"\"\"\n",
    "        batch, src_len = src.shape[0], src.shape[1]\n",
    "        \n",
    "        # create position tensor, shape will be [batch, src_len] by dooing so batch_first will be True\n",
    "        position  = torch.arange(start=0, end=src_len, device=device).unsqueeze(0).repeat(batch, 1)\n",
    "        \n",
    "        # embeddings\n",
    "        tok_embedded = self.tok_embedding(src)\n",
    "        pos_embedded = self.pos_embedding(position)\n",
    "        \n",
    "        # scale the token embeddings by multiplyig it with srqt(d_model) where d_model is embedding_dim\n",
    "        tok_scaled = tok_embedded * self.scale.to(device)\n",
    "        \n",
    "        # add the scaled_tok and position embedding and then apply dropout, that will be input to the encoder\n",
    "        encoder_input = self.dropout(tok_scaled + pos_embedded)\n",
    "        \n",
    "        print(encoder_input.shape)\n",
    "        \n",
    "        #feed the input to the encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(encoder_input, None)\n",
    "        \n",
    "        return src\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=len(source.vocab)\n",
    "embedding_dim=256\n",
    "num_layers=1\n",
    "n_heads=8\n",
    "pf_dim=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    vocab_size=vocab_size, \n",
    "    embedding_dim=embedding_dim, \n",
    "    num_layers=num_layers,\n",
    "    n_heads=n_heads,\n",
    "    pf_dim=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 27, 256])\n",
      "query.shpe torch.Size([128, 27, 256]) key.shape torch.Size([128, 27, 256])  value.shape torch.Size([128, 27, 256])\n",
      "Q.shape torch.Size([128, 27, 256]) K.shape torch.Size([128, 27, 256]) V.shape torch.Size([128, 27, 256])\n"
     ]
    }
   ],
   "source": [
    "outputs = encoder(batch.src, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 27, 256])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, n_heads, pf_dim, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # layer normalization\n",
    "        self.layer_norm =  nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # multi-head attention (I love this layer)\n",
    "        self.multihead_attention = MultiheadAttention(embedding_dim, n_heads, dropout)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # feedforward layer\n",
    "        self.positionwise_ff = PositionwiseFeedForwardLayer(embedding_dim, pf_dim, dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        \n",
    "        # compute the attention values (query, key, value) -> (src, src, src)\n",
    "        \n",
    "        attn_out, _  = self.multihead_attention(src, src, src, src_mask)\n",
    "        \n",
    "        \n",
    "        # Normalize the attention and build residual connection and then pass it to positionwise ff layer followed by LN\n",
    "        attn_norm_out = self.layer_norm(src + self.dropout(attn_out))\n",
    "        ff_out = self.positionwise_ff(attn_norm_out)\n",
    "        \n",
    "        ff_norm_out = self.layer_norm(attn_norm_out + self.dropout(ff_out)) \n",
    "        # this will be output of the Transformer's Encoder layer\n",
    "        # ff_norm_out.shape [batch, seq_len, embedding_dim]\n",
    "        \n",
    "        \n",
    "        return attn_out\n",
    "#         return ff_norm_out     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = EncoderLayer(embedding_dim=embedding_dim, n_heads=n_heads, pf_dim=pf_dim, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shpe torch.Size([128, 31, 256]) key.shape torch.Size([128, 31, 256])  value.shape torch.Size([128, 31, 256])\n",
      "Q.shape torch.Size([128, 31, 256]) K.shape torch.Size([128, 31, 256]) V.shape torch.Size([128, 31, 256])\n",
      "torch.Size([128, 31, 256])\n"
     ]
    }
   ],
   "source": [
    "attn = encoder_layer(outputs)\n",
    "print(attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "        Scaled dot product attention\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, n_heads, dropout):\n",
    "        \"\"\"\n",
    "            n_heads > 0\n",
    "        \"\"\"\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = embedding_dim // n_heads\n",
    "        \n",
    "        # fc for key, query, values\n",
    "        self.fc_k  = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.fc_q = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.fc_v = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        \n",
    "        self.fc_o = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.scale = torch.sqrt(torch.FloatTensor([embedding_dim])).to(device)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "            query.shape -> [batch, src_len, embedding_dim]\n",
    "            key.shape -> [batch, src_len, embedding_dim]\n",
    "            value.shape -> [batch, src_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        src_len = query.shape[1]\n",
    "        \n",
    "        print(f'query.shpe {query.shape} key.shape {key.shape}  value.shape {value.shape}')\n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #Q = [batch size, query len, hid dim] K&V would have same dim\n",
    "                \n",
    "        print(f'Q.shape {Q.shape} K.shape {K.shape} V.shape {V.shape}')\n",
    "        Q = Q.view(batch_size, src_len, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, src_len, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, src_len, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        #Q = [batch size, n heads, query len, head dim] K&V have to have same dim\n",
    "        \n",
    "                \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #energy = [batch size, n heads, query len, key len]\n",
    "        \n",
    "        \n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        attention = torch.softmax(energy, dim = -1) \n",
    "        #attention = [batch size, n heads, query len, key len]\n",
    "                \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #x = [batch size, n heads, query len, embedding_dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        #x = [batch size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.embedding_dim)\n",
    "        #x = [batch size, query len, embedding_dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch size, query len, embedding_dim]\n",
    "        \n",
    "        return x, attention    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = MultiheadAttention(embedding_dim=embedding_dim, n_heads=n_heads, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shpe torch.Size([128, 34, 256]) key.shape torch.Size([128, 34, 256])  value.shape torch.Size([128, 34, 256])\n",
      "Q.shape torch.Size([128, 34, 256]) K.shape torch.Size([128, 34, 256]) V.shape torch.Size([128, 34, 256])\n"
     ]
    }
   ],
   "source": [
    "x, attn = attention(outputs, outputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 8, 34, 34])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positionwise Feedforad Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, pf_dim, dropout):\n",
    "        super(PositionwiseFeedForwardLayer, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=embedding_dim, out_features=pf_dim)\n",
    "        self.fc2 = nn.Linear(in_features=pf_dim, out_features=embedding_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "            x.shape -> [batch, src_len, embedding_dim]\n",
    "        \"\"\"\n",
    "        out = self.fc2(self.dropout(F.relu((self.fc1(x)))))\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
