{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import torch\n",
    "from util import count_parameters\n",
    "import random\n",
    "\n",
    "# data\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "# model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# training\n",
    "import tqdm\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the fields\n",
    "SRC = Field(\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en\",\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "downloading training.tar.gz\ntraining.tar.gz: 100%|██████████| 1.21M/1.21M [00:08<00:00, 135kB/s]\ndownloading validation.tar.gz\nvalidation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 103kB/s]\ndownloading mmt_task1_test2016.tar.gz\nmmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 101kB/s]\n"
    }
   ],
   "source": [
    "# download the dataset\n",
    "train, val, test = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "SRC.build_vocab(train)\n",
    "TRG.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_loader, val_loader, test_loader = BucketIterator.splits(\n",
    "    datasets=(train, val, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([64, 27]) torch.Size([64, 25])\n"
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.src.shape, batch.trg.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Encoder hidden_dim and decoder hidden_dim will be same to avoid discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch.src\n",
    "y = batch.trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # gru layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        # input_features = 2*hidden_dim, output_features\n",
    "        self.fc = nn.Linear(in_features=2*hidden_size, out_features=hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x.shape -> [batch, seq_len]\n",
    "        \n",
    "        # compute the embedding\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded.shape -> [batch, seq_len, embedding_dim]\n",
    "        \n",
    "        # pass the embedding to gru layer\n",
    "        outputs, hidden = self.gru(embedded)   \n",
    "        # outputs.shape -> [batch, seq_len, 2*hidden_size] and hidden.shape -> [2, batch, hidden_size]\n",
    "        \n",
    "        # hidden contains both forward pass hidden state as well backward pass hidden state concat the both\n",
    "        concated = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        \n",
    "        # new hidden state \n",
    "        hidden = torch.tanh(self.fc(concated))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        \n",
    "        \"\"\"It is advised to use same encoder_hidden_dim and decoder_hidden_dim\"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # attention layer and params\n",
    "        self.attn = nn.Linear(in_features=(3*hidden_size), out_features=hidden_size)\n",
    "        self.v = nn.Linear(in_features=hidden_size, out_features=1, bias=False)\n",
    "        \n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        # hidden.shape -> [batch, hidden_dim]\n",
    "        # encoder_outptus.shape -> [batch, seq_len, 2*hidden_dim]\n",
    "        seq_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  \n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze()\n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, attention, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = attention\n",
    "        \n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        # reccurent net\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=(2*hidden_size)+embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(in_features=3*hidden_size+embedding_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        \"\"\"\n",
    "            input.size() -> [batch, 1] - At time t only one token of each sample will be decoded\n",
    "            hidden.size() ->[batch, hidden_size]\n",
    "            encoder_outputs -> [batch, seq_len, 2*hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.embedding(input.to(device))\n",
    "        # embedded[batch, seq_len, embedding_dim] embedded will have only two dim if seq_len is 1 (ie: at time t)\n",
    "             \n",
    "        attn = self.attention(hidden, encoder_outputs)\n",
    "        attn = attn.unsqueeze(1)\n",
    "    \n",
    "        # calculate the weighted sum\n",
    "        weighted = torch.bmm(attn, encoder_outputs)\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "#         print(f'weighted shape {weighted.shape} embedded shape {embedded.shape}')\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        output, hidden = self.gru(rnn_input, hidden.unsqueeze())\n",
    "        \n",
    "        # prepare the input for fully connected layer and make predicitons\n",
    "        fc_input = torch.cat((weighted, output, embedded), dim=2)\n",
    "        prediction = self.fc(fc_input)\n",
    "        \n",
    "        return prediction, hidden.squeeze()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio=0.25):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "            trg.size() -> [batch, seq_len]\n",
    "            src.size() -> [batch, seq_len]\n",
    "        \"\"\"\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        batch, seq_len, vocab_size =  trg.shape[0], trg.shape[1], self.decoder.vocab_size\n",
    "        \n",
    "        outputs = torch.zeros((batch, seq_len, vocab_size), device=device)\n",
    "        \n",
    "        # take the first token of each samples in the batch and calculate the attention for the same\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            output, hidden = decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output.squeeze()\n",
    "\n",
    "            # is teacher force\n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.squeeze().argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data, criterion):\n",
    "    loss, ppl = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data:\n",
    "            outputs = model(batch.src, batch.trg)\n",
    "            batch_size, seq_len = outputs.size(0), outputs.size(1)\n",
    "            l = criterion(outputs.view(batch_size*seq_len, -1).contiguous().to(device), batch.trg.view(-1))\n",
    "            p = torch.exp(l)\n",
    "            loss.append(l.item())\n",
    "            ppl.append(p.item())\n",
    "    return sum(loss)/len(loss), sum(ppl)/len(ppl)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = len(SRC.vocab)\n",
    "trg_vocab = len(TRG.vocab)\n",
    "hidden_size = 512 # same for encoder and decoder\n",
    "embedding_dim =  256 # same for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder =  Encoder(vocab_size=src_vocab, embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)\n",
    "attention = Attention(hidden_size=hidden_size).to(device)\n",
    "decoder = Decoder(attention, vocab_size=trg_vocab, embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)\n",
    "model = Seq2Seq(encoder=encoder, decoder=decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Seq2Seq(\n  (encoder): Encoder(\n    (embedding): Embedding(18658, 256)\n    (gru): GRU(256, 512, batch_first=True, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=512, bias=True)\n    (dropout): Dropout(p=0.15, inplace=False)\n  )\n  (decoder): Decoder(\n    (attention): Attention(\n      (attn): Linear(in_features=1536, out_features=512, bias=True)\n      (v): Linear(in_features=512, out_features=1, bias=False)\n    )\n    (embedding): Embedding(9797, 256)\n    (gru): GRU(1280, 512, batch_first=True)\n    (fc): Linear(in_features=1792, out_features=9797, bias=True)\n  )\n)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------+\n",
      "|             Modules              | Parameters |\n",
      "+----------------------------------+------------+\n",
      "|     encoder.embedding.weight     |  4776960   |\n",
      "|     encoder.gru.weight_ih_l0     |   393216   |\n",
      "|     encoder.gru.weight_hh_l0     |   786432   |\n",
      "|      encoder.gru.bias_ih_l0      |    1536    |\n",
      "|      encoder.gru.bias_hh_l0      |    1536    |\n",
      "| encoder.gru.weight_ih_l0_reverse |   393216   |\n",
      "| encoder.gru.weight_hh_l0_reverse |   786432   |\n",
      "|  encoder.gru.bias_ih_l0_reverse  |    1536    |\n",
      "|  encoder.gru.bias_hh_l0_reverse  |    1536    |\n",
      "|        encoder.fc.weight         |   524288   |\n",
      "|         encoder.fc.bias          |    512     |\n",
      "|  decoder.attention.attn.weight   |   786432   |\n",
      "|   decoder.attention.attn.bias    |    512     |\n",
      "|    decoder.attention.v.weight    |    512     |\n",
      "|     decoder.embedding.weight     |  2508544   |\n",
      "|     decoder.gru.weight_ih_l0     |  1966080   |\n",
      "|     decoder.gru.weight_hh_l0     |   786432   |\n",
      "|      decoder.gru.bias_ih_l0      |    1536    |\n",
      "|      decoder.gru.bias_hh_l0      |    1536    |\n",
      "|        decoder.fc.weight         |  17559808  |\n",
      "|         decoder.fc.bias          |    9799    |\n",
      "+----------------------------------+------------+\n",
      "Total Trainable Params: 31288391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31288391"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "total_steps = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Steps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 0/10 | Train_loss 5.6659 | Train_ppl 412.2913 | Val_loss 5.1656 | Val_ppl 177.7235\nSteps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 1/10 | Train_loss 4.6013 | Train_ppl 103.7042 | Val_loss 4.2714 | Val_ppl 74.1122\nSteps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 2/10 | Train_loss 3.8116 | Train_ppl 46.0777 | Val_loss 3.8905 | Val_ppl 50.7565\nSteps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 3/10 | Train_loss 3.2173 | Train_ppl 25.3009 | Val_loss 3.7378 | Val_ppl 43.9298\nSteps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 4/10 | Train_loss 2.7660 | Train_ppl 16.0822 | Val_loss 3.7351 | Val_ppl 43.7428\nSteps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 5/10 | Train_loss 2.4687 | Train_ppl 11.9568 | Val_loss 3.7069 | Val_ppl 43.8507\nSteps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 6/10 | Train_loss 2.2636 | Train_ppl 9.7247 | Val_loss 3.8148 | Val_ppl 47.2787\nSteps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 7/10 | Train_loss 2.0964 | Train_ppl 8.2190 | Val_loss 3.9363 | Val_ppl 54.5961\nSteps:   0%|          | 0/454 [00:00<?, ?it/s]Epoch 8/10 | Train_loss 1.9336 | Train_ppl 6.9833 | Val_loss 3.8481 | Val_ppl 49.4791\nEpoch: 100%|██████████| 10/10 [18:42<00:00, 114.41s/it]Epoch 9/10 | Train_loss 1.7752 | Train_ppl 5.9518 | Val_loss 4.0037 | Val_ppl 58.4978\n"
    }
   ],
   "source": [
    "steps = 0\n",
    "\n",
    "# epoch progress bar\n",
    "epoch_progress = tqdm.tqdm(total=epochs, desc=\"Epoch\", position=0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # step progress bar\n",
    "    step_progress = tqdm.tqdm(total=total_steps, desc=\"Steps\", position=0)\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_ppl = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        \n",
    "        outputs = model(batch.src, batch.trg)\n",
    "        \n",
    "        batch_size, seq_len = outputs.shape[0], outputs.shape[1]\n",
    "        \n",
    "        outputs = outputs.view((batch_size*seq_len, -1))\n",
    "        labels = batch.trg.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        ppl = torch.exp(loss)\n",
    "        \n",
    "        # backprograpage the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_ppl.append(ppl.item())\n",
    "        \n",
    "        if steps % 200 == 0:\n",
    "            print(f'Steps {steps}/{total_steps} | Train_loss {loss.item():.4f} | Train_ppl {ppl.item():.4f}')\n",
    "        steps += 1\n",
    "        step_progress.update(1)\n",
    "    \n",
    "    avg_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    avg_ppl = sum(epoch_ppl)/len(epoch_ppl)\n",
    "    \n",
    "    val_loss, val_ppl = eval(model, val_loader, criterion)\n",
    "    print(f'Epoch {epoch}/{epochs} | Train_loss {avg_loss:.4f} | Train_ppl {avg_ppl:.4f} | Val_loss {val_loss:.4f} | Val_ppl {val_ppl:.4f}')\n",
    "    epoch_progress.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save, Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test_loss 4.0117 | Test_PPL 59.965221643447876\n"
    }
   ],
   "source": [
    "loss, ppl = eval(trained, test_loader, criterion)\n",
    "print(f'Test_loss {loss:.4f} | Test_PPL {ppl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}