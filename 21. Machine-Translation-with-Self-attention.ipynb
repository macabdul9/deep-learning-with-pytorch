{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import torch\n",
    "from util import count_parameters\n",
    "import random\n",
    "\n",
    "# data\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "# model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# training\n",
    "import tqdm\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_len = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the fields\n",
    "SRC = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"de\",\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "TRG = Field(\n",
    "    init_token=\"<sos>\",\n",
    "    eos_token=\"<eos>\",\n",
    "    lower=True,\n",
    "    tokenize=\"spacy\",\n",
    "    tokenizer_language=\"en\",\n",
    "    batch_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "train, val, test = Multi30k.splits(\n",
    "    exts=(\".de\", \".en\"),\n",
    "    fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocab\n",
    "SRC.build_vocab(train)\n",
    "TRG.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders\n",
    "train_loader, val_loader, test_loader = BucketIterator.splits(\n",
    "    datasets=(train, val, test),\n",
    "    batch_sizes=(batch_size, batch_size, batch_size),\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 26]) torch.Size([64, 26])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch.src.shape, batch.trg.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "- Encoder hidden_dim and decoder hidden_dim will be same to avoid discrepancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch.src\n",
    "y = batch.trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "        # gru layer\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        # input_features = 2*hidden_dim, output_features\n",
    "        self.fc = nn.Linear(in_features=2*hidden_size, out_features=hidden_size)\n",
    "        self.dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x.shape -> [batch, seq_len]\n",
    "        \n",
    "        # compute the embedding\n",
    "        embedded = self.embedding(x)\n",
    "        # embedded.shape -> [batch, seq_len, embedding_dim]\n",
    "        \n",
    "        # pass the embedding to gru layer\n",
    "        outputs, hidden = self.gru(embedded)   \n",
    "        # outputs.shape -> [batch, seq_len, 2*hidden_size] and hidden.shape -> [2, batch, hidden_size]\n",
    "        \n",
    "        # hidden contains both forward pass hidden state as well backward pass hidden state concat the both\n",
    "        concated = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        \n",
    "        # new hidden state \n",
    "        hidden = torch.tanh(self.fc(concated))\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self-attention Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size):\n",
    "        \n",
    "        \"\"\"It is advised to use same encoder_hidden_dim and decoder_hidden_dim\"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        # attention layer and params\n",
    "        self.attn = nn.Linear(in_features=(3*hidden_size), out_features=hidden_size)\n",
    "        self.v = nn.Linear(in_features=hidden_size, out_features=1, bias=False)\n",
    "        \n",
    "    \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        # hidden.shape -> [batch, hidden_dim]\n",
    "        # encoder_outptus.shape -> [batch, seq_len, 2*hidden_dim]\n",
    "        seq_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_len, 1)  \n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze()\n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, attention, vocab_size, embedding_dim, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = attention\n",
    "        \n",
    "        \n",
    "        # embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n",
    "        # reccurent net\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=(2*hidden_size)+embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(in_features=3*hidden_size+embedding_dim, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \n",
    "        \"\"\"\n",
    "            input.size() -> [batch, 1] - At time t only one token of each sample will be decoded\n",
    "            hidden.size() ->[batch, hidden_size]\n",
    "            encoder_outputs -> [batch, seq_len, 2*hidden_size]\n",
    "        \"\"\"\n",
    "        \n",
    "        embedded = self.embedding(input.to(device))\n",
    "        # embedded[batch, seq_len, embedding_dim] embedded will have only two dim if seq_len is 1 (ie: at time t)\n",
    "             \n",
    "        attn = self.attention(hidden, encoder_outputs)\n",
    "        attn = attn.unsqueeze(1)\n",
    "    \n",
    "        # calculate the weighted sum\n",
    "        weighted = torch.bmm(attn, encoder_outputs)\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "#         print(f'weighted shape {weighted.shape} embedded shape {embedded.shape}')\n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        \n",
    "        output, hidden = self.gru(rnn_input)\n",
    "        \n",
    "        # prepare the input for fully connected layer and make predicitons\n",
    "        fc_input = torch.cat((weighted, output, embedded), dim=2)\n",
    "        prediction = self.fc(fc_input)\n",
    "        \n",
    "        return prediction, hidden.squeeze()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, teacher_forcing_ratio=0.25):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "    \n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "            trg.size() -> [batch, seq_len]\n",
    "            src.size() -> [batch, seq_len]\n",
    "        \"\"\"\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        batch, seq_len, vocab_size =  trg.shape[0], trg.shape[1], self.decoder.vocab_size\n",
    "        \n",
    "        outputs = torch.zeros((batch, seq_len, vocab_size), device=device)\n",
    "        \n",
    "        # take the first token of each samples in the batch and calculate the attention for the same\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            output, hidden = decoder(input, hidden, encoder_outputs)\n",
    "            outputs[:, t] = output.squeeze()\n",
    "\n",
    "            # is teacher force\n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < self.teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.squeeze().argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, data, criterion):\n",
    "    loss, ppl = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data:\n",
    "            outputs = model(batch.src, batch.trg)\n",
    "            batch_size, seq_len = outputs.size(0), outputs.size(1)\n",
    "            l = criterion(outputs.view(batch_size*seq_len, -1).contiguous().to(device), batch.trg.view(-1))\n",
    "            p = torch.exp(l)\n",
    "            loss.append(l.item())\n",
    "            ppl.append(p.item())\n",
    "    return sum(loss)/len(loss), sum(ppl)/len(ppl)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = len(SRC.vocab)\n",
    "trg_vocab = len(TRG.vocab)\n",
    "hidden_size = 512 # same for encoder and decoder\n",
    "embedding_dim =  256 # same for encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder =  Encoder(vocab_size=src_vocab, embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)\n",
    "attention = Attention(hidden_size=hidden_size).to(device)\n",
    "decoder = Decoder(attention, vocab_size=trg_vocab, embedding_dim=embedding_dim, hidden_size=hidden_size).to(device)\n",
    "model = Seq2Seq(encoder=encoder, decoder=decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(18660, 256)\n",
       "    (gru): GRU(256, 512, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.15, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(9799, 256)\n",
       "    (gru): GRU(1280, 512, batch_first=True)\n",
       "    (fc): Linear(in_features=1792, out_features=9799, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------+\n",
      "|             Modules              | Parameters |\n",
      "+----------------------------------+------------+\n",
      "|     encoder.embedding.weight     |  4776960   |\n",
      "|     encoder.gru.weight_ih_l0     |   393216   |\n",
      "|     encoder.gru.weight_hh_l0     |   786432   |\n",
      "|      encoder.gru.bias_ih_l0      |    1536    |\n",
      "|      encoder.gru.bias_hh_l0      |    1536    |\n",
      "| encoder.gru.weight_ih_l0_reverse |   393216   |\n",
      "| encoder.gru.weight_hh_l0_reverse |   786432   |\n",
      "|  encoder.gru.bias_ih_l0_reverse  |    1536    |\n",
      "|  encoder.gru.bias_hh_l0_reverse  |    1536    |\n",
      "|        encoder.fc.weight         |   524288   |\n",
      "|         encoder.fc.bias          |    512     |\n",
      "|  decoder.attention.attn.weight   |   786432   |\n",
      "|   decoder.attention.attn.bias    |    512     |\n",
      "|    decoder.attention.v.weight    |    512     |\n",
      "|     decoder.embedding.weight     |  2508544   |\n",
      "|     decoder.gru.weight_ih_l0     |  1966080   |\n",
      "|     decoder.gru.weight_hh_l0     |   786432   |\n",
      "|      decoder.gru.bias_ih_l0      |    1536    |\n",
      "|      decoder.gru.bias_hh_l0      |    1536    |\n",
      "|        decoder.fc.weight         |  17559808  |\n",
      "|         decoder.fc.bias          |    9799    |\n",
      "+----------------------------------+------------+\n",
      "Total Trainable Params: 31288391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31288391"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "total_steps = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(device)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 1/454 [00:00<02:25,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 0/454 | Train_loss 9.1900 | Train_ppl 9798.4600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  44%|████▍     | 201/454 [00:48<01:09,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 200/454 | Train_loss 5.3987 | Train_ppl 221.1098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  88%|████████▊ | 401/454 [01:42<00:14,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 400/454 | Train_loss 5.2952 | Train_ppl 199.3795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].97s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10 | Train_loss 5.5405 | Train_ppl 378.7909 | Val_loss 5.1971 | Val_ppl 183.9508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  32%|███▏      | 147/454 [00:44<01:26,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 600/454 | Train_loss 5.0191 | Train_ppl 151.2736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  76%|███████▋  | 347/454 [01:49<00:37,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 800/454 | Train_loss 4.9254 | Train_ppl 137.7439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].40s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train_loss 4.8522 | Train_ppl 130.4754 | Val_loss 4.6669 | Val_ppl 110.1469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  20%|██        | 93/454 [00:30<02:05,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 1000/454 | Train_loss 4.1686 | Train_ppl 64.6269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  65%|██████▍   | 293/454 [01:28<00:47,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 1200/454 | Train_loss 4.0167 | Train_ppl 55.5161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].99s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train_loss 4.0960 | Train_ppl 61.6530 | Val_loss 3.7980 | Val_ppl 46.9670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   9%|▊         | 39/454 [00:10<01:52,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 1400/454 | Train_loss 3.7101 | Train_ppl 40.8581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  53%|█████▎    | 239/454 [01:06<01:04,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 1600/454 | Train_loss 3.3538 | Train_ppl 28.6109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  97%|█████████▋| 439/454 [02:04<00:03,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 1800/454 | Train_loss 3.3611 | Train_ppl 28.8223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].58s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train_loss 3.4940 | Train_ppl 33.3905 | Val_loss 3.6220 | Val_ppl 38.8636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  41%|████      | 185/454 [00:53<01:11,  3.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 2000/454 | Train_loss 3.1117 | Train_ppl 22.4589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  85%|████████▍ | 385/454 [01:52<00:21,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 2200/454 | Train_loss 3.3167 | Train_ppl 27.5679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].91s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train_loss 3.0847 | Train_ppl 22.1661 | Val_loss 3.4998 | Val_ppl 34.6025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  29%|██▉       | 131/454 [00:37<01:31,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 2400/454 | Train_loss 2.7832 | Train_ppl 16.1706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  73%|███████▎  | 331/454 [01:37<00:36,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 2600/454 | Train_loss 2.9247 | Train_ppl 18.6294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].94s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train_loss 2.7419 | Train_ppl 15.7333 | Val_loss 3.4048 | Val_ppl 31.8850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  17%|█▋        | 77/454 [00:24<01:52,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 2800/454 | Train_loss 2.3164 | Train_ppl 10.1389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  61%|██████    | 277/454 [01:30<01:17,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 3000/454 | Train_loss 2.6070 | Train_ppl 13.5578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].63s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train_loss 2.4778 | Train_ppl 12.0515 | Val_loss 3.3929 | Val_ppl 31.4242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   5%|▌         | 23/454 [00:08<02:47,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 3200/454 | Train_loss 2.2414 | Train_ppl 9.4064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  49%|████▉     | 223/454 [01:18<01:30,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 3400/454 | Train_loss 2.2448 | Train_ppl 9.4387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  93%|█████████▎| 423/454 [02:33<00:11,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 3600/454 | Train_loss 2.5149 | Train_ppl 12.3654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].96s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train_loss 2.2866 | Train_ppl 9.9442 | Val_loss 3.4511 | Val_ppl 33.5790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  37%|███▋      | 169/454 [01:05<01:38,  2.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 3800/454 | Train_loss 2.2173 | Train_ppl 9.1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  81%|████████▏ | 369/454 [02:17<00:26,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 4000/454 | Train_loss 2.3647 | Train_ppl 10.6408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:   0%|          | 0/454 [00:00<?, ?it/s].31s/it]s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train_loss 2.1417 | Train_ppl 8.5995 | Val_loss 3.4507 | Val_ppl 33.4235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  25%|██▌       | 115/454 [00:41<01:42,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 4200/454 | Train_loss 1.8659 | Train_ppl 6.4615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Steps:  69%|██████▉   | 315/454 [01:55<00:42,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps 4400/454 | Train_loss 1.8664 | Train_ppl 6.4653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 10/10 [24:07<00:00, 157.17s/it]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train_loss 2.0027 | Train_ppl 7.4876 | Val_loss 3.4755 | Val_ppl 34.7025\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "\n",
    "# epoch progress bar\n",
    "epoch_progress = tqdm.tqdm(total=epochs, desc=\"Epoch\", position=0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # step progress bar\n",
    "    step_progress = tqdm.tqdm(total=total_steps, desc=\"Steps\", position=0)\n",
    "    \n",
    "    epoch_loss = []\n",
    "    epoch_ppl = []\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        \n",
    "        outputs = model(batch.src, batch.trg)\n",
    "        \n",
    "        batch_size, seq_len = outputs.shape[0], outputs.shape[1]\n",
    "        \n",
    "        outputs = outputs.view((batch_size*seq_len, -1))\n",
    "        labels = batch.trg.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        ppl = torch.exp(loss)\n",
    "        \n",
    "        # backprograpage the loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        epoch_loss.append(loss.item())\n",
    "        epoch_ppl.append(ppl.item())\n",
    "        \n",
    "        if steps % 200 == 0:\n",
    "            print(f'Steps {steps}/{total_steps} | Train_loss {loss.item():.4f} | Train_ppl {ppl.item():.4f}')\n",
    "        steps += 1\n",
    "        step_progress.update(1)\n",
    "    \n",
    "    avg_loss = sum(epoch_loss)/len(epoch_loss)\n",
    "    avg_ppl = sum(epoch_ppl)/len(epoch_ppl)\n",
    "    \n",
    "    val_loss, val_ppl = eval(model, val_loader, criterion)\n",
    "    print(f'Epoch {epoch}/{epochs} | Train_loss {avg_loss:.4f} | Train_ppl {avg_ppl:.4f} | Val_loss {val_loss:.4f} | Val_ppl {val_ppl:.4f}')\n",
    "    epoch_progress.update(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save, Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macab/miniconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Seq2Seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/home/macab/miniconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type Attention. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = torch.load(\"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_loss 3.4197 | Test_PPL 33.099823236465454\n"
     ]
    }
   ],
   "source": [
    "loss, ppl = eval(trained, test_loader, criterion)\n",
    "print(f'Test_loss {loss:.4f} | Test_PPL {ppl}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda1b4d65181bfe435290e55078ed6e0090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}