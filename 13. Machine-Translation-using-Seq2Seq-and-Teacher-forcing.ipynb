{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Machine-Translation-with-Seq2Seq-and-Teacher-forcing.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6miyNebmSfT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torchtext\n",
        "import tqdm\n",
        "from torch.utils import tensorboard"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THE-vBXK_RYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTZ8XhP2TEvE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install torchtext==0.6.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSZXWl26TOzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python -m spacy download en\n",
        "# !python -m spacy download de"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1ZI1M_PSfT6",
        "colab_type": "text"
      },
      "source": [
        "### Data\n",
        "- Multi30K dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rJ98C8SfT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUzgkr-KSfT-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5517d98e-5dec-414f-fa39-f14b513beff1"
      },
      "source": [
        "device"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rv7ftN2SfUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SRC Field\n",
        "source = torchtext.data.Field(\n",
        "    init_token=\"<sos>\",\n",
        "    eos_token=\"<eos>\",\n",
        "    tokenize=\"spacy\",\n",
        "    tokenizer_language=\"de\",\n",
        "    batch_first=True,\n",
        "    lower=True\n",
        ")\n",
        "\n",
        "\n",
        "# TRG Field\n",
        "target = torchtext.data.Field(\n",
        "    init_token=\"<sos>\",\n",
        "    eos_token=\"<eos>\",\n",
        "    tokenize=\"spacy\",\n",
        "    tokenizer_language=\"en\",\n",
        "    batch_first=True,\n",
        "    lower=True\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upTr_yYVSfUF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "44443640-4909-4930-f428-bb19ead50812"
      },
      "source": [
        "train, valid, test = torchtext.datasets.Multi30k.splits(\n",
        "    exts=(\".de\", \".en\"),\n",
        "    fields=(source, target)\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rtraining.tar.gz:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 4.96MB/s]\n",
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 1.38MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n",
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 1.34MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_HBhB2OSfUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "source.build_vocab(train, min_freq=2)\n",
        "target.build_vocab(train, min_freq=2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCZqrTHpSfUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader, test_loader, val_loader = torchtext.data.BucketIterator.splits(\n",
        "    datasets=(train, test, valid), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jUABapWSfUO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d496d08-dd9e-46ed-a3d2-a365516fa365"
      },
      "source": [
        "for batch in train_loader:\n",
        "    print(batch.src.size())\n",
        "    break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 23])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMNIwqZTSfUR",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m5M_XsuSfUR",
        "colab_type": "text"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB8C2kerSfUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=4, dropout = 0.25):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # transoform the int tokens into embedding\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "        \n",
        "        # reccurent layer\n",
        "        self.seq = nn.LSTM(\n",
        "            input_size = embedding_dim,\n",
        "            hidden_size = hidden_dim,\n",
        "            num_layers = num_layers,\n",
        "            batch_first = True,\n",
        "            dropout = dropout,\n",
        "            bidirectional = True\n",
        "            \n",
        "        )\n",
        "    \n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "            outputs -> is the output at each time-steps, if the bidirectional is True it will concatenated\n",
        "            hidden -> hidden state of the last time step\n",
        "            cell -> cell state at the last time step\n",
        "        \"\"\"\n",
        "        \n",
        "        embedded =  self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.seq(embedded)\n",
        "        return hidden, cell\n",
        "    "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yc9jfSRwSfUV",
        "colab_type": "text"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyZ48K8tSfUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers = 4, dropout = 0.25):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # get the embedding of the int tokens\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim\n",
        "        )\n",
        "        \n",
        "        # recurrent layer\n",
        "        self.seq = nn.LSTM(\n",
        "            input_size = embedding_dim,\n",
        "            hidden_size = hidden_dim,\n",
        "            num_layers = num_layers,\n",
        "            batch_first = True,\n",
        "            dropout = dropout,\n",
        "            bidirectional = True\n",
        "        )\n",
        "        \n",
        "        # outputs should be from each time step, since it will be applied to each \n",
        "        # in_features is double of embedding dim because reccurent layer is bidirection and both gets concatenated\n",
        "        self.fc = nn.Linear(in_features=2*hidden_dim, out_features=vocab_size)\n",
        "        \n",
        "    \n",
        "    def forward(self, trg, hidden, cell):\n",
        "        \"\"\" We have to pass the hidden state and cell state of the encoder network to decoder dims should be the same\"\"\"\n",
        "        embedded = self.embedding(trg)\n",
        "        outputs, (_, _) = self.seq(embedded, (hidden, cell)) ## here only outputs is relevant \n",
        "        prediction = self.fc(outputs.squeeze())\n",
        "        return prediction\n",
        "        "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzpyfOeHSfUY",
        "colab_type": "text"
      },
      "source": [
        "#### Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ntn5SiXSfUY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder, lr=1e-3, teacher_forcing_ratio=0.25):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "        self.lr = lr\n",
        "    \n",
        "    \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        batch_size = trg.size(0)\n",
        "        trg_seq_size = trg.size(1)\n",
        "        \n",
        "        outputs = torch.zeros((batch_size, trg_seq_size, self.decoder.vocab_size))\n",
        "        \n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        # hidden and cell size-> [2*num_layers, batch, hidden_dim] to [batch, 2*num_layers, hidden_dim]\n",
        "        hidden = hidden.permute(1, 0, 2)\n",
        "        cell = cell.permute(1, 0, 2)\n",
        "        \n",
        "        input = trg[0]\n",
        "        \n",
        "        for t in range(1, trg_seq_size):\n",
        "            output = self.decoder(input.unsqueeze(0).contiguous(), hidden[t-1].unsqueeze(1).contiguous(), cell[t-1].unsqueeze(1).contiguous())\n",
        "            outputs[t-1] = output\n",
        "            \n",
        "            # is teacher force\n",
        "            teacher_force = torch.rand(1).item() < self.teacher_forcing_ratio\n",
        "            top1 = torch.argmax(output, 1)\n",
        "            \n",
        "            input = trg[t] if teacher_force else top1\n",
        "            \n",
        "        output = self.decoder(input.unsqueeze(0).contiguous(), hidden[trg_seq_size-1].unsqueeze(1).contiguous(), cell[trg_seq_size-1].unsqueeze(1).contiguous())\n",
        "        outputs[trg_seq_size-1] = output  \n",
        "        return outputs\n",
        "    "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-qXIsPpSfUb",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB6UYFuqSfUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model, data, criterion):\n",
        "    loss, ppl = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in data:\n",
        "            outputs = model(batch.src, batch.trg)\n",
        "            batch_size, seq_len = outputs.size(0), outputs.size(1)\n",
        "            \n",
        "            l = criterion(outputs.view(batch_size*seq_len, -1).contiguous().to(device), batch.trg.view(-1))\n",
        "            p = torch.exp(l)\n",
        "            \n",
        "            loss.append(l.item())\n",
        "            ppl.append(p.item())\n",
        "    return sum(loss)/len(loss), sum(ppl)/len(ppl)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it7_9IbLSfUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(vocab_size=len(source.vocab), embedding_dim=100, hidden_dim=64).to(device)\n",
        "decoder = Decoder(vocab_size=len(target.vocab), embedding_dim=100, hidden_dim=64).to(device)\n",
        "model = Seq2Seq(encoder=encoder, decoder=decoder).to(device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGKJA-JbSfUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-3\n",
        "epochs = 10\n",
        "PAD_IDX = target.vocab.stoi[target.pad_token]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu0g5XJ-SfUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optimizer\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(device)\n",
        "writer = tensorboard.SummaryWriter()"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2I9eo8xSfUl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "e829e6a7-dbe8-4985-876c-755cdd767384"
      },
      "source": [
        "epoch_progress = tqdm.tqdm(total=epochs, desc=\"Epoch\", position=0)\n",
        "steps = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    step_progress = tqdm.tqdm(total=len(train_loader), desc=\"Step\", position=0)\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        \n",
        "        # compute  the outputs\n",
        "        outputs = model(batch.src, batch.trg)\n",
        "        batch_size, seq_len  = outputs.size(0), outputs.size(1)\n",
        "        \n",
        "        # compute the loss, ppl, gradient and backpropagate the loss\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(outputs.view(batch_size*seq_len, -1).contiguous().to(device), batch.trg.view(-1).contiguous())\n",
        "        ppl = torch.exp(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute the validaiton loss and ppl\n",
        "        val_loss, val_ppl = eval(model, val_loader, criterion)\n",
        "        \n",
        "        if steps%200 ==0:\n",
        "            print(f'Epoch {epoch} | Steps {steps} | Train_loss {loss.item():.4f} | Train_PPL {ppl.item():.4f} | Val_loss {val_loss:.4f} | Val_PPL {val_ppl:.4f}')\n",
        "        \n",
        "        writer.add_scalar(\"train_loss\", loss.item(), steps)\n",
        "        writer.add_scalar(\"train_ppl\", ppl.item(), steps)\n",
        "        writer.add_scalar(\"val_loss\", val_loss, steps)\n",
        "        writer.add_scalar(\"val_ppl\", val_ppl, steps)\n",
        "\n",
        "        steps += 1\n",
        "        step_progress.update(1)\n",
        "        \n",
        "    epoch_progress.update(1)\n",
        "        \n",
        "    \n",
        "    "
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step:   0%|          | 1/454 [00:01<14:05,  1.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 | Steps 0 | Train_loss 8.3769 | Train_PPL 4345.3335 | Val_loss 8.3967 | Val_PPL 4465.9990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Step:  44%|████▍     | 201/454 [06:30<08:05,  1.92s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 | Steps 200 | Train_loss 6.7218 | Train_PPL 830.3455 | Val_loss 7.5428 | Val_PPL 1987.8233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Step:  88%|████████▊ | 401/454 [12:52<01:38,  1.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 | Steps 400 | Train_loss 6.7529 | Train_PPL 856.5743 | Val_loss 7.4994 | Val_PPL 1907.5278\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Step:  95%|█████████▌| 433/454 [13:52<00:41,  1.99s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-315fe704460e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# compute  the outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c32a44cbf565>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mteacher_force\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtop1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrg_seq_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrg_seq_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for dimension 0 with size 8"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UunZ1tLhTuOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "fad01180-1f71-404e-b10c-8c63ef8455b6"
      },
      "source": [
        "out = model(batch.src, batch.trg)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-b6233ed4ff31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c32a44cbf565>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mtop1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mteacher_force\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtop1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrg_seq_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrg_seq_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 8 is out of bounds for dimension 0 with size 8"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ODSN3H8ST2qd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "482d6aee-da46-4008-9905-b8141caca1d4"
      },
      "source": [
        "batch.trg.shape"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rj9oHYdcuXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96f230dd-5c11-4353-c489-6153a1fb25b4"
      },
      "source": [
        "batch.src.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 23])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKdfQheMcyOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}